<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Julian Eisenschlos]]></title><description><![CDATA[Machine Learning, Natural Language Processing, Math • AI Research @ Google • Co-founder botmaker.com • Previously Facebook & ASAPP]]></description><link>https://eisenjulian.github.io/</link><image><url>https://eisenjulian.github.io/favicon.png</url><title>Julian Eisenschlos</title><link>https://eisenjulian.github.io/</link></image><generator>Ghost 3.30</generator><lastBuildDate>Sat, 10 Apr 2021 11:52:32 GMT</lastBuildDate><atom:link href="https://eisenjulian.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Learning to Reason Over Tables from Less Data]]></title><description><![CDATA[In "Understanding tables with intermediate pre-training", published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data.]]></description><link>https://eisenjulian.github.io/learning-to-reason-over-tables-from-less-data/</link><guid isPermaLink="false">607189b208a58e183495e165</guid><category><![CDATA[Natural Language Processing]]></category><dc:creator><![CDATA[Julian Eisenschlos]]></dc:creator><pubDate>Sat, 10 Apr 2021 11:44:58 GMT</pubDate><media:content url="https://eisenjulian.github.io/content/images/2021/04/few-shot.png" medium="image"/><content:encoded><![CDATA[<img src="https://eisenjulian.github.io/content/images/2021/04/few-shot.png" alt="Learning to Reason Over Tables from Less Data"><p><em>The task of recognizing <a href="https://en.wikipedia.org/wiki/Textual_entailment">textual entailment</a>, also known as natural language inference, consists of determining whether a piece of text (a premise), can be implied or contradicted (or neither) by another piece of text (the hypothesis). While this problem is often considered an important test for the reasoning skills of machine learning (ML) systems and has been studied in depth for plain text inputs, much less effort has been put into applying such models to structured data, such as websites, tables, databases, etc. Yet, recognizing textual entailment is especially relevant whenever the contents of a table need to be accurately summarized and presented to a user, and is essential for high fidelity <a href="https://en.wikipedia.org/wiki/Question_answering">question answering</a> systems and <a href="https://en.wikipedia.org/wiki/Virtual_assistant">virtual assistants</a>.</em></p><p><em>In "<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.27/">Understanding tables with intermediate pre-training</a>", published in <a href="https://2020.emnlp.org/papers/findings">Findings of EMNLP 2020</a>, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data. We build upon our earlier <a href="https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html">TAPAS</a> model, which was an extension of the <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">BERT</a> bi-directional <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> model with special embeddings to find answers in tables. Applying our new pre-training objectives to TAPAS yields a new state of the art on multiple datasets involving tables. On <a href="https://tabfact.github.io/">TabFact</a>, for example, it reduces the gap between model and human performance by ~50%. We also systematically benchmark methods of selecting relevant input for higher efficiency, achieving 4x gains in speed and memory, while retaining 92% of the results. All the models for different tasks and sizes are released on <a href="https://github.com/google-research/tapas">GitHub repo</a>, where you can try them out yourself in a <a href="http://tiny.cc/tapas-tabfact-colab">colab</a> Notebook.</em></p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://ai.googleblog.com/2021/01/learning-to-reason-over-tables-from.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Learning to Reason Over Tables from Less Data</div><div class="kg-bookmark-description">Posted by Julian Eisenschlos, AI Resident, Google Research, Zürich The task of recognizing textual entailment , also known as natural la...</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ai.googleblog.com/favicon.ico" alt="Learning to Reason Over Tables from Less Data"><span class="kg-bookmark-publisher">Google AI Blog</span></div></div><div class="kg-bookmark-thumbnail"><img src="http://2.bp.blogspot.com/-qRz-hnwUdY4/WulXSQ6Rv4I/AAAAAAAATvQ/shk7KsphA0c3E3nUMsDVASqYaH0PhLPNwCK4BGAYYCw/s1600/GoogleAI_logo_horizontal_color_rgb.png" alt="Learning to Reason Over Tables from Less Data"></div></a><figcaption>Cross posted from the Google AI blog</figcaption></figure><hr><h3 id="textual-entailment">Textual Entailment</h3><p>The task of textual entailment is more challenging when applied to tabular data than plain text. Consider, for example, a table from Wikipedia with some sentences derived from its associated table content. Assessing if the content of the table entails or contradicts the sentence may require looking over multiple columns and rows, and possibly performing simple numeric computations, like averaging, summing, differencing, etc.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-TOSTbdc9ndk/YBRIuPjyCZI/AAAAAAAAHEw/NYIj9usvgaQu1Fm9UfBJPL_12M6Y0F5fgCLcBGAsYHQ/w400-h184/image5.png" class="kg-image" alt="Learning to Reason Over Tables from Less Data"><figcaption>A table together with some statements from <a href="https://tabfact.github.io/">TabFact</a>. The content of the table can be used to support or contradict the statements.</figcaption></figure><p>Following the methods used by TAPAS, we encode the content of a statement and a table together, pass them through a Transformer model, and obtain a single number with the probability that the statement is entailed or refuted by the table.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-zaz8_g9XvxI/YBRJAP21w4I/AAAAAAAAHE4/CvLDXDybaiQs9QNwEdcwW6KaLvyUeJUewCLcBGAsYHQ/w400-h255/TAPAS.jpg" class="kg-image" alt="Learning to Reason Over Tables from Less Data"><figcaption>The <a href="https://www.aclweb.org/anthology/2020.acl-main.398">TAPAS model</a> architecture uses a BERT model to encode the statement and the flattened table, read row by row. Special embeddings are used to encode the table structure. The vector output of the first token is used to predict the probability of entailment.</figcaption></figure><p>Because the only information in the training examples is a binary value (i.e., "correct" or "incorrect"), training a model to understand whether a statement is entailed or not is challenging and highlights the difficulty in achieving generalization in deep learning, especially when the provided training signal is scarce. Seeing isolated entailed or refuted examples, a model can easily pick-up on spurious patterns in the data to make a prediction, for example the presence of the word "tie" in "Greg Norman and Billy Mayfair tie in rank", instead of truly comparing their ranks, which is what is needed to successfully apply the model beyond the original training data.</p><h3 id="pre-training-tasks">Pre-training Tasks</h3><p>Pre-training tasks can be used to “warm-up” models by providing them with large amounts of readily available unlabeled data. However, pre-training typically includes primarily plain text and not tabular data. In fact, TAPAS was originally pre-trained using a simple masked language modelling objective that was not designed for tabular data applications. In order to improve the model performance on tabular data, we introduce two novel pretraining binary-classification tasks called <em>counterfactual</em> and <em>synthetic</em>, which can be applied as a second stage of pre-training (often called <em>intermediate pre-training</em>).</p><p>In the counterfactual task, we source sentences from Wikipedia that mention an entity (person, place or thing) that also appears in a given table. Then, 50% of the time, we modify the statement by swapping the entity for another alternative. To make sure the statement is realistic, we choose a replacement among the entities in the same column in the table. The model is trained to recognize whether the statement was modified or not. This pre-training task includes millions of such examples, and although the reasoning about them is not complex, they typically will still sound natural.</p><p>For the synthetic task, we follow a method similar to <a href="https://www.aclweb.org/anthology/P15-1129/">semantic parsing</a> in which we generate statements using a simple set of grammar rules that require the model to understand basic mathematical operations, such as sums and averages (e.g., "the sum of earnings"), or to understand how to filter the elements in the table using some condition (e.g.,"the country is Australia"). Although these statements are artificial, they help improve the numerical and logical reasoning skills of the model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-kLdqZMbPSEY/YBRJIn_88tI/AAAAAAAAHE8/MBhRtGDgqKIATplbDWbad6sRniHgjmLhgCLcBGAsYHQ/w400-h75/image1.png" class="kg-image" alt="Learning to Reason Over Tables from Less Data"><figcaption>Example instances for the two novel pre-training tasks. <strong>Counterfactual</strong> examples swap entities mentioned in a sentence that accompanies the input table for a plausible alternative. <strong>Synthetic</strong> statements use grammar rules to create new sentences that require combining the information of the table in complex ways.</figcaption></figure><h3 id="results">Results</h3><p>We evaluate the success of the counterfactual and synthetic pre-training objectives on the TabFact dataset by comparing to the baseline TAPAS model and to two prior models that have exhibited success in the textual entailment domain, <a href="https://www.aclweb.org/anthology/2020.acl-main.539/">LogicalFactChecker</a> (LFC) and <a href="https://www.aclweb.org/anthology/2020.emnlp-main.126/">Structure Aware Transformer</a> (SAT). The baseline TAPAS model exhibits improved performance relative to LFC and SAT, but the pre-trained model (TAPAS+CS) performs significantly better, achieving a new state of the art.</p><p>We also apply TAPAS+CS to question answering tasks on the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253">SQA dataset</a>, which requires that the model find answers from the content of tables in a dialog setting. The inclusion of CS objectives improves the previous best performance by more than 4 points, demonstrating that this approach also generalizes performance beyond just textual entailment.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-gRLgwMzDwyQ/YBRJcOPxUFI/AAAAAAAAHFI/cVl0tAGeEX4TLCLt-NvUS7Nfq_FBHy81QCLcBGAsYHQ/w400-h229/Accuracy.png" class="kg-image" alt="Learning to Reason Over Tables from Less Data"><figcaption>Results on TabFact (<strong>left</strong>) and SQA (<strong>right</strong>). Using the synthetic and counterfactual datasets, we achieve new state-of-the-art results in both tasks by a large margin.</figcaption></figure><h3 id="data-and-compute-efficiency">Data and Compute Efficiency</h3><p>Another aspect of the counterfactual and synthetic pre-training tasks is that since the models are already tuned for binary classification, they can be applied without any fine-tuning to TabFact. We explore what happens to each of the models when trained only on a subset (or even none) of the data. Without looking at a single example, the TAPAS+CS model is competitive with a strong baseline Table-Bert, and when only 10% of the data are included, the results are comparable to the previous state-of-the-art.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-ij2gSWPrt8U/YBRJkWHIfVI/AAAAAAAAHFM/NZWkx9y0f94Gg_FjBLH0-MOXFHYiJANGgCLcBGAsYHQ/w400-h236/image7.png" class="kg-image" alt="Learning to Reason Over Tables from Less Data"><figcaption>Dev accuracy on TabFact relative to the fraction of the training data used.</figcaption></figure><p>A general concern when trying to use large models such as this to operate on tables, is that their high computational requirements makes it difficult for them to parse very large tables. To address this, we investigate whether one can heuristically select subsets of the input to pass through the model in order to optimize its computational efficiency.</p><p>We conducted a systematic study of different approaches to filter the input and discovered that simple methods that select for word overlap between a full column and the subject statement give the best results. By dynamically selecting which tokens of the input to include, we can use fewer resources or work on larger inputs at the same cost. The challenge is doing so without losing important information and hurting accuracy.</p><p>For instance, the models discussed above all use sequences of 512 tokens, which is around the normal limit for a transformer model (although recent efficiency methods like the <a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html">Reformer</a> or <a href="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html">Performer</a> are proving effective in scaling the input size). The column selection methods we propose here can allow for faster training while still achieving high accuracy on TabFact. For 256 input tokens we get a very small drop in accuracy, but the model can now be pre-trained, fine-tuned and make predictions up to two times faster. With 128 tokens the model still outperforms the previous state-of-the-art model, with an even more significant speed-up — 4x faster across the board.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-LjZgvsrnh7E/YBRJpZUYYUI/AAAAAAAAHFQ/-b-_V4JCJq0S4S_k6vNcqVlRSvfPx6WwwCLcBGAsYHQ/w400-h236/image6.png" class="kg-image" alt="Learning to Reason Over Tables from Less Data"><figcaption>Accuracy on TabFact using different sequence lengths, by shortening the input with our column selection method.</figcaption></figure><p>Using both the column selection method we proposed and the novel pre-training tasks, we can create table parsing models that need fewer data and less compute power to obtain better results.</p><p>We have made available the new models and pre-training techniques at <a href="https://github.com/google-research/tapas">our GitHub repo</a>, where you can try it out yourself in <a href="http://tiny.cc/tapas-tabfact-colab">colab</a>. In order to make this approach more accessible, we also shared models of varying sizes all the way down to “<a href="https://arxiv.org/abs/1908.08962">tiny</a>”. It is our hope that these results will help spur development of table reasoning among the broader research community.</p><h3 id="acknowledgements">Acknowledgements</h3><p><em>This work was carried out by Julian Martin Eisenschlos, Syrine Krichene and Thomas Müller from our <a href="https://research.google/teams/language/">Language Team</a> in Zürich. We would like to thank Jordan Boyd-Graber, Yasemin Altun, Emily Pitler, Benjamin Boerschinger, Srini Narayanan, Slav Petrov, William Cohen and Jonathan Herzig for their useful comments and suggestions.</em></p>]]></content:encoded></item><item><title><![CDATA[Efficient multi-lingual language model fine-tuning]]></title><description><![CDATA[Our latest paper studies multilingual text classification and introduces MultiFiT, a novel method based on ULMFiT. MultiFiT, trained on 100 labeled documents in the target language, outperforms multi-lingual BERT, and the LASER algorithm—even though LASER requires a corpus of parallel texts.]]></description><link>https://eisenjulian.github.io/multifit/</link><guid isPermaLink="false">5c9cded58e0072434003f2d4</guid><category><![CDATA[Natural Language Processing]]></category><dc:creator><![CDATA[Julian Eisenschlos]]></dc:creator><pubDate>Tue, 10 Sep 2019 00:00:00 GMT</pubDate><media:content url="https://eisenjulian.github.io/content/images/2020/08/multifit_bootstrapping.png" medium="image"/><content:encoded><![CDATA[<img src="https://eisenjulian.github.io/content/images/2020/08/multifit_bootstrapping.png" alt="Efficient multi-lingual language model fine-tuning"><p><em>Most of the world’s text is not in English. To enable researchers and practitioners to build impactful solutions in their domains, understanding how our NLP architectures fare in many languages needs to be more than an afterthought. In this post, we introduce our latest paper that studies multilingual text classification and introduces MultiFiT, a novel method based on <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a>. MultiFiT, trained on 100 labeled documents in the target language, outperforms <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multi-lingual BERT</a>. It also outperforms the cutting-edge <a href="https://engineering.fb.com/ai-research/laser-multilingual-sentence-embeddings/">LASER</a> algorithm—even though LASER requires a corpus of parallel texts, and MultiFiT does not.</em></p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://nlp.fast.ai/classification/2019/09/10/multifit.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Efficient multi-lingual language model fine-tuning · fast.ai NLP</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://nlp.fast.ai/favicon.png" alt="Efficient multi-lingual language model fine-tuning"></div></div><div class="kg-bookmark-thumbnail"><img src="https://nlp.fast.ai/images/multifit_vocabularies.png" alt="Efficient multi-lingual language model fine-tuning"></div></a><figcaption>Cross posted from the fast AI blog</figcaption></figure><hr><p><em>This is joint work by Sebastian Ruder, Piotr Czapla, Marcin Kardas, Sylvain Gugger, Jeremy Howard, and Julian Eisenschlos and benefits from the <a href="https://forums.fast.ai/t/language-model-zoo-gorilla/14623">hundreds</a> of <a href="https://forums.fast.ai/t/multilingual-ulmfit/28117">insights</a> into multilingual transfer learning from the whole <a href="https://forums.fast.ai/">fast.ai forum community</a>. We invite you to read <a href="https://arxiv.org/abs/1909.04761">the full EMNLP 2019 paper</a> or check out the code <a href="https://github.com/n-waves/ulmfit-multilingual">here</a>.</em></p><hr><h2 id="introduction">Introduction</h2><p>If you have ever worked on an NLP task in any language other than English, we feel your pain. The last couple of years have brought <a href="http://ruder.io/nlp-imagenet/">impressive progress</a> in deep learning-based approaches for natural language processing tasks and there’s much to be excited about. However, those advances can be slow to transfer beyond English. In the past, most of academia showed little interest in publishing research or building datasets that go beyond the English language, even though industry applications desperately need language-agnostic techniques. Luckily, thanks to efforts around democratizing access to machine learning and initiatives such as the <a href="https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/">Bender rule</a>, the tides are changing.</p><p>Existing approaches for cross-lingual NLP rely on either:</p><ul><li>Parallel data across languages—that is, a corpus of documents with exactly the same contents, but written in different languages. This is very hard to acquire in a general setting.</li><li>A shared vocabulary—that is, a vocabulary that is common across multiple languages. This approach over-represents languages with a lot of data. For some examples, have a look at <a href="http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html">this blog post</a> An example is <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a>, which is very resource-intensive to train, and can struggle when languages are dissimilar.</li></ul><p>The main appeal of cross-lingual models like multilingual BERT are their <em>zero-shot transfer capabilities</em>: given only labels in a high-resource language such as English, they can transfer to another language without any training data in that language. We argue that many low-resource applications do not provide easy access to training data in a high-resource language. Such applications include disaster response on social media, help desks that deal with community needs or support local business owners, etc. In such settings, it is often easier to collect a few hundred training examples in the low-resource language. The utility of zero-shot approaches in general is quite limited; by definition, if you are applying a model to some language, then you have some documents in that language. So it makes sense to use them to help train your model!</p><p>In addition, when the target language is very different to the source language (most often English), zero-shot transfer may perform poorly or fail altogether. We have seen this with <a href="https://arxiv.org/abs/1805.03620">cross-lingual word embeddings</a> and more recently for <a href="https://arxiv.org/abs/1906.01502">multilingual BERT</a>.</p><p>We show that we can fine-tune efficient monolingual language models that are competitive with multilingual BERT, in many languages, on a few hundred examples. Our proposed approach Multilingual Fine-Tuning (MultiFiT) is different in a number of ways from the current main stream of NLP models: We do not build on BERT, but leverage a more efficient variant of an LSTM architecture. Consequently, our approach is much cheaper to pretrain and more efficient in terms of space and time complexity. Lastly, we emphasize having nimble monolingual models vs. a monolithic cross-lingual one. We also show that we can achieve superior zero-shot transfer by using a cross-lingual model as the teacher. This highlights the potential of combining monolingual and cross-lingual information.</p><h2 id="our-approach">Our approach</h2><p>Our method is based on <a href="https://arxiv.org/abs/1801.06146">Universal Language Model Fine-Tuning (ULMFiT)</a>. For more context, we invite you to check out the <a href="https://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html">previous blog post</a> that explains it in depth. MultiFiT extends ULMFiT to make it more efficient and more suitable for language modelling beyond English: It utilizes tokenization based on subwords rather than words and employs a QRNN rather than an LSTM. In addition, it leverages a number of other improvements.</p><p><strong>Subword tokenization</strong>   ULMFiT uses word-based tokenization, which works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish. Some languages such as Chinese don’t really even have the concept of a “word”, so require heuristic segmentation approaches, which tend to be complicated, slow, and inaccurate. On the other extreme as can be seen below, character-based models use individual characters as tokens. While in this case the vocabulary (and thus the number of parameters) can be small, such models require modelling longer dependencies and can thus be harder to train and less expressive than word-based models.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://nlp.fast.ai/images/multifit_vocabularies.png" class="kg-image" alt="Efficient multi-lingual language model fine-tuning"><figcaption>From character-based to word-based tokenization.</figcaption></figure><p>To mitigate this, similar to current neural machine translation models and pretrained language models like BERT and GPT-2, we employ <a href="https://www.aclweb.org/anthology/D18-2012.pdf">SentencePiece subword tokenization</a>, which has since been incorporated into the <a href="https://docs.fast.ai/text.data.html#SPProcessor">fast.ai text package</a>. Subword tokenization strikes a balance between the two approaches by using a mixture of character, subword and word tokens, depending on how common they are.</p><p>This way we can have short (on average) representations of sentences, yet are still able to encode rare words. We use a unigram language model based on Wikipedia that learns a vocabulary of tokens together with their probability of occurrence. It assumes that tokens occur independently (hence the unigram in the name). During tokenization this method finds the most probable segmentation into tokens from the vocabulary. In the image below we show an example of tokenizing “_subwords” using a vocabulary trained on English Wikipedia (“_” is used by SentencePiece to denote a whitespace).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://nlp.fast.ai/images/multifit_bpe_tokenization_path.png" class="kg-image" alt="Efficient multi-lingual language model fine-tuning"><figcaption>A graph of possible subword tokenizations for the token '_subwords'. The number next to each token is its negative log likelihood. The most probable tokenization corresponds to the shortest weighted path connecting the blue nodes (indicated in red).</figcaption></figure><p>To sum up, subword tokenization has two very desirable properties for multilingual language modelling:</p><ol><li>Subwords more easily represent inflections, including common prefixes and suffixes and are thus well-suited for morphologically rich languages.</li><li>Subword tokenization is a good fit for open-vocabulary problems and eliminates out-of-vocabulary tokens, as the coverage is close to 100% tokens.</li></ol><p><strong>QRNN</strong> ULMFiT used a state-of-the-art language model at the time, the <a href="https://arxiv.org/abs/1708.02182">AWD-LSTM</a>. The AWD-LSTM is a regular LSTM with tuned dropout hyper-parameters. While recent state-of-the-art language models have been increasingly based on Transformers, such as the <a href="https://arxiv.org/abs/1901.02860">Transformer-XL</a>, <a href="https://arxiv.org/abs/1909.01792">recurrent models still seem to have the edge on smaller datasets</a> such as the Penn Treebank and WikiText-2.</p><p>To make our model more efficient, we replace the AWD-LSTM with a <a href="https://arxiv.org/abs/1611.01576">Quasi-Recurrent Neural Network (QRNN)</a>. The QRNN strikes a balance between an CNN and an LSTM: It can be parallelized across time and minibatch dimensions like a CNN and inherits the LSTM’s sequential bias as the output depends on the order of elements in the sequence. Specifically, the QRNN alternates convolutional layers, which are parallel across timesteps and a recurrent pooling function, which is parallel across channels.</p><p>We can see in the figure below how it differs from an LSTM and a CNN. In the LSTM, computation at each timestep depends on the results from the previous timestep (indicated by the non-continuous blocks), while CNNs and QRNNs are more easily parallelizable (indicated by the continuous blocks).</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://nlp.fast.ai/images/multifit_qrnn.png" class="kg-image" alt="Efficient multi-lingual language model fine-tuning"><figcaption>The computation structure of the QRNN compared with an LSTM and a CNN (Bradbury et al., 2019)</figcaption></figure><p>In our experiments, we obtain a 2-3x speed-up during training using QRNNs. QRNNs have been used in a number of applications, such as <a href="https://arxiv.org/abs/1705.08947">state-of-the-art speech recognition</a> in the past.</p><p><strong>Other improvements</strong> Instead of using ULMFiT’s slanted triangular learning rate schedule and gradual unfreezing, we achieve faster training and convergence by employing a cosine variant of the <a href="https://sgugger.github.io/the-1cycle-policy.html">one-cycle policy</a> that is available in the <a href="https://docs.fast.ai/callbacks.one_cycle.html">fast.ai library</a>. Finally, we use <a href="https://rickwierenga.com/blog/fast.ai/FastAI2019-12.html">label smoothing</a>, which transforms the one-hot labels to a “smoother” distribution and has been found particularly useful when learning from noisy labels.</p><p>ULMFiT ensembles the predictions of a forward and backward language model. Even though <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5a8fba10ed_1_114">bidirectionality has been found to be important</a> in contextual word vectors, we did not see big improvements for our downstream tasks (text classification) with <a href="https://aclweb.org/anthology/N18-1202/">ELMo-style joint training</a>. As joint training is quite memory-intensive and we emphasize efficiency, we opted to just train forward language models for all languages.</p><p>The full model can be seen in the below figure. It consists of a subword embedding layer, four QRNN layers, an aggregation layer, and two linear layers. The aggregation and linear layers are the same as used in ULMFiT.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://nlp.fast.ai/images/multifit_architecture.png" class="kg-image" alt="Efficient multi-lingual language model fine-tuning"><figcaption>The MultiFiT language model with a classifier head. The dimensionality of each layer can be seen in each box at the top (Figure 1 in the paper).</figcaption></figure><h2 id="results">Results</h2><p>We compare our model to state-of-the-art cross-lingual models including <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a> and <a href="https://arxiv.org/abs/1812.10464">LASER</a> (which uses parallel sentences) on two multilingual document classification datasets. Perhaps surprisingly, we find that our monolingual language models fine-tuned only on 100 labeled examples of the corresponding task in the target language outperform zero-shot inference (trained on 1000 examples in the source language) with multilingual BERT and LASER. MultiFit also outperforms the other methods when all models are fine-tuned on 1000 target language examples.</p><p>For the detailed results, have a look at <a href="https://arxiv.org/abs/1909.04761">the paper</a>.</p><h2 id="zero-shot-transfer-with-a-cross-lingual-teacher">Zero-shot Transfer with a Cross-lingual Teacher</h2><p>Still, if a powerful cross-lingual model and labeled data in a high-resource language are available, it would be nice to make use of them in some way. To this end, we propose to use the classifier that is learned on top of the cross-lingual model on the source language data as a teacher to obtain labels for training our model on the target language. This way, we can perform zero-shot transfer using our monolingual language model by bootstrapping from a cross-lingual one.</p><p>To illustrate how this works, take a look at the following diagram:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://nlp.fast.ai/images/multifit_bootstrapping.png" class="kg-image" alt="Efficient multi-lingual language model fine-tuning"><figcaption>The steps of the cross-lingual bootstrapping method for zero-shot cross-lingual transfer (Figure 2 in the paper).</figcaption></figure><p>The process consists of three main steps:</p><ol><li>Our monolingual language model is pre-trained on Wikipedia data in the target language (a) and fine-tuned on in-domain data of the corresponding task (b).</li><li>We now train a classifier on top of cross-lingual model such as LASER using labelled data in a high-resource source language and perform zero-shot inference as usual with this classifier to predict labels on target language documents.</li><li>In a the final step (c), we can now use these predicted labels to fine-tune a classifier on top of our fine-tuned monolingual language model.</li></ol><p>This is similar to <a href="https://arxiv.org/abs/1503.02531">distillation</a>, which has recently been used to <a href="https://arxiv.org/abs/1909.00100">train smaller</a> <a href="https://arxiv.org/abs/1910.01108">language models</a> or <a href="https://arxiv.org/abs/1903.12136">distill task-specific information into downstream models</a>. In contrast, to previous work, we do not just seek to distill the information of a big model into a small model but into one with a <em>different inductive bias</em>. In addition to circumventing the need for labels in the target language, our approach thus brings another benefit: As the monolingual model is specialized to the target language, its inductive bias might be more suitable than the more language-agnostic representations learned by the cross-lingual model. It might thus be able to make better use of labels in the target language, even if they are noisy.</p><p>We obtain evidence for this hypothesis as the monolingual language model fine-tuned on zero-shot predictions outperforms its teacher in all settings.</p><h2 id="robustness-to-noise">Robustness to Noise</h2><p>Another hypothesis why this teaching works so well is that pre-training makes the monolingual language robust to noise to some extent. The pre-trained information stored in the model may act as a regularizer, biasing it towards the correct labels that are in line with its knowledge of the language.</p><p>To test this, we compare a pre-trained language model with a non-pre-trained language model that are fine-tuned on 1k or 10k labelled examples where labels are perturbed with a probability ranging from 0 to 0.75 in the below diagram.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://nlp.fast.ai/images/multifit_noise_comparison.png" class="kg-image" alt="Efficient multi-lingual language model fine-tuning"><figcaption>Comparison of MultiFiT's robustness to label noise with and without pretraining. The red line shows the theoretical accuracy of a perfect model that achieves 100% accuracy with all labels.</figcaption></figure><p>As we can see, the pre-trained models are much more robust to label noise. Even with 30% noisy labels, they still maintain about the same performance, whereas the performance of the models without pre-training quickly decays. This highlights robustness to noise as an additional benefit of transfer learning and may facilitate faster crowd-sourcing and data annotation.</p><h2 id="next-steps">Next Steps</h2><p>We are initially releasing seven pre-trained language models in German, Spanish, French, Italian, Japanese, Russian, and Chinese, since they are the ones in the datasets we studied. You can find the code <a href="https://github.com/n-waves/ulmfit-multilingual">here</a>. We hope to release many many more, with the help of the community.</p><p>The fast.ai community has been very helpful in collecting datasets in many more languages, and applying MultiFiT to them—nearly always with state-of-the-art results. For space limitations in the paper those datasets were not included, and we opted to select well used, balanced multi-lingual datasets. Special thanks to <a href="https://aayux.github.io/">Aayush Yadav</a>, <a href="https://twitter.com/aDemyan4uk">Alexey Demyanchuk</a>, <a href="https://github.com/benjaminvdb">Benjamin van der Burgh</a>, <a href="https://github.com/cahya-wirawan">Cahya Wirawan</a>, <a href="https://github.com/cstorm125">Charin Polpanumas</a>, <a href="https://twitter.com/NirantK">Nirant Kasliwal</a> and <a href="https://github.com/tpietruszka">Tomasz Pietruszka</a>.</p><p>Another interesting question to explore further is how very low-resource languages or dialects can benefit from larger corpora in similar languages. We are looking forward to seeing what problems you apply MultiFiT to, so don’t hesitate to ask and share your results in the <a href="https://forums.fast.ai/">fast.ai forums</a>.</p>]]></content:encoded></item><item><title><![CDATA[Neural Networks in 100 lines of pure Python]]></title><description><![CDATA[Can we build a Deep learning framework in plain Python and Numpy? Can we make it compact, clear and extendable? Let's set out to explore those ideas and see what we can create!]]></description><link>https://eisenjulian.github.io/deep-learning-in-100-lines/</link><guid isPermaLink="false">5c8e67ae6d2ff81194b67a1c</guid><category><![CDATA[Automatic Differentiation]]></category><category><![CDATA[Math]]></category><category><![CDATA[Tutorials]]></category><dc:creator><![CDATA[Julian Eisenschlos]]></dc:creator><pubDate>Sun, 17 Mar 2019 15:33:37 GMT</pubDate><media:content url="https://eisenjulian.github.io/content/images/2019/03/onion.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://eisenjulian.github.io/content/images/2019/03/onion.jpg" alt="Neural Networks in 100 lines of pure Python"><p><em>Can we build a Deep learning framework in pure Python and Numpy? Can we make it compact, clear and extendable? Let's set out to explore those ideas and see what we can create!</em></p><hr><p>In today's day and age, there are multiple frameworks to choose from, with various important features such as auto-differentiation, graph-based optimized computation, and hardware acceleration. It's easy to take those features for granted, but every once in a while peeking under the hood can teach us what makes things work and what doesn't.</p><p>What we'll cover:</p><ul><li>Automatic back-propagation</li><li>How to implement a few basic layers</li><li>How to create a training loop</li></ul><p>If you want to jump straight to the code, feel free to scroll to the second half of the post or try it live in this <a href="https://colab.research.google.com/github/eisenjulian/slides/blob/master/NN_from_scratch/notebook.ipynb">Colab Notebook</a>.</p><p>The design choices are heavily based on <a href="https://github.com/explosion/thinc">Thinc</a>'s Deep Learning library since they came up with the smart idea of using  closures and the stack to share and keep track of intermediate results while calculating gradients. This post is also inspired by Jeremy Howard's great <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">PyTorch tutorial</a>, but taking it one more step towards the bottom getting rid of <code>autograd</code>.</p><h2 id="a-word-on-notation">A word on notation</h2><p>Notation can often be a source of confusion, but it can also help us develop the right intuition. In the context of calculus for back propagation, we can focus on functions or on variables to think about derivatives. These are the edges and nodes in the computational graph. We will get to the same results either way, but I find that focusing on variables helps to make things more natural.</p><ul><li>Let $f:\mathbb{R}^n\to\mathbb{R}$ and $x\in\mathbb{R}^n$, then the gradient is the $n$-dimensional row vector of partial derivatives $\frac{\partial f}{\partial_j }(x)$</li><li>If $f:\mathbb{R}^n\to\mathbb{R}^m$ and $x\in\mathbb{R}^n$ then the Jacobian matrix is an $m\times n$ matrix such that the $i^{\text{th}}$ is the gradient of $f$ restricted to the $i$ coordinate:<br>$$Jf(x)_{ij} = \frac{\partial f_i}{\partial_j}(x)$$</li><li>For a function $f$ and two vectors $a$ and $b$ such that $a = f(b)$, whenever $f$ is clear from context we can write $\frac{\partial a}{\partial b}$ to denote the Jacobian $Jf$, or gradient if $a$ is a real number.</li></ul><h2 id="the-chain-rule">The chain rule</h2><p>If we have three vectors in three vector spaces $a\in A$, $b\in B$, and $c\in C$ and two differentiable functions $f:A\to B$ and $g:B\to C$ such that $f(a) = b$ and $g(b) = c$ we can get the Jacobian of the composition as a matrix product of the Jacobian of $f$ and $g$:</p><p>$$\frac{\partial c}{\partial a} = \frac{\partial c}{\partial b} \cdot \frac{\partial b}{\partial a}$$</p><p>That's the chain rule in all its glory. The back-propagation algorithm, originally described in the '60s and '70s is the application of the chain rule to calculate gradients of a real function with respect to its various parameters.</p><p>Keep in mind that our end goal is to find a minimum (hopefully global) of a function by taking steps in the opposite direction of the said gradient, because locally at least this will take it downwards. This is how it looks when we have two parameters to optimize.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://eisenjulian.github.io/content/images/2019/03/gradient_descent.gif" class="kg-image" alt="Neural Networks in 100 lines of pure Python"><figcaption>Source: <a href="https://jed-ai.github.io/py1_gd_animation/">7 Simple Steps To Visualize And Animate The Gradient Descent Algorithm</a></figcaption></figure><h2 id="reverse-mode-differentiation">Reverse mode differentiation</h2><p>If we compose many functions $f_i(a_i) = a_{i+1}$ instead of just two, then we can apply that formula repeatedly and conclude that</p><p>$$\frac{\partial a_n}{\partial a_1} = \frac{\partial a_n}{\partial a_{n-1}} \cdots \frac{\partial a_3}{\partial a_2}\cdot\frac{\partial a_2}{\partial a_1}$$</p><p>Of the many ways we can compute this product, the most common are from left to right or from right to left. </p><p>In practice, if $a_n$ is a scalar we will calculate the full gradient by starting with the row vector $\frac{\partial a_n}{\partial a_{n-1}}$ and multiplying by the Jacobians $\frac{\partial a_i}{\partial a_{i-1}}$ on the right one at a time. This operation is sometimes referred as <em>VJP</em>, or <em>Vector-Jacobian Product</em>. The whole process is known as <strong>reverse mode</strong> differentiation, because as intermediate results we gradually compute the gradients working from the last one $\frac{\partial a_n}{\partial a_{n-1}}$ backward through $\frac{\partial a_n}{\partial a_{n-2}}$, $\frac{\partial a_n}{\partial a_{n-3}}$, etc. In the end, we know the gradient of $a_n$ with respect to all other variables.</p><p>$$\frac{\partial a_n}{\partial a_{n-1}} \rightarrow \frac{\partial a_n}{\partial a_{n-2}} \rightarrow \cdots \rightarrow \frac{\partial a_n}{\partial a_1}$$</p><p>In contrast, <strong>forward mode</strong> does the exact opposite. It starts from one Jacobian like $\frac{\partial a_2}{\partial a_1}$ and multiplies on the left by $\frac{\partial a_3}{\partial a_2}$ to calculate $\frac{\partial a_3}{\partial a_1}$. If we keep multiplying by $\frac{\partial a_i}{\partial a_{i-1}}$ we'll gradually get the derivatives of all other variables with respect to $a_1$. When $a_1$ is a scalar, all the matrixes on the right side of the products are column vectors, and this is called a <em>Jacobian-Vector Product</em> (or <em>JVP</em>).</p><p>$$\frac{\partial a_n}{\partial a_1} \leftarrow\cdots\leftarrow \frac{\partial a_2}{\partial a_1} \leftarrow  \frac{\partial a_2}{\partial a_1}$$</p><p>For back propagation, as you might have guessed, we are interested in the first of this two, since there's a single value we want on top, the loss function, derived with respect to multiple variables, the weights. Forward mode could still be used to compute those gradients, but it would be far less efficient since we would have to repeat the process multiple times.</p><p>This sounds like a lot of high-dimensional matrix products, but the trick is that very often the Jacobian matrixes will be sparse, <a href="https://en.wikipedia.org/wiki/Block_matrix">block</a> or even diagonal and, because we only care about the result of multiplying it by a row vector on the left, we won't need to compute or store it explicitly</p><p>In this article, we are focusing on a sequential composition of layers, but the same ideas apply to any algorithm or computational graph that we use to get to the result. Yet another nice description of reverse and forward mode in a more general context is given <a href="http://colah.github.io/posts/2015-08-Backprop/">here</a>.</p><h2 id="link-to-deep-neural-networks">Link to deep neural networks</h2><p>In the typical setting for supervised machine learning, we have a big complex function that takes in a tensor of numerical features for our labeled samples, and several tensors that correspond to weights that characterize the model.</p><p>The loss, as a scalar function of both the samples and weights, is a measure of how far the model output was from the expected labels. We want to minimize it to find the most suitable weights. In deep learning, this function is expressed as a composition of simpler functions, each of which is easy to differentiate. All of them but the last are what we refer to as layers, and each layer usually has two sets of parameters: the inputs (which can be the output of the previous layer) and the weights. </p><p>The last function is the loss metric, which also has two sets of parameters: model output $y$ and the true labels $\hat{y}$. For example, if the loss metric $l$ is the mean square error then $\frac{\partial l}{\partial y}$ is $2\  \text{avg}(y - \hat{y})$. The gradient of the loss will be the starting row vector to apply reverse mode differentiation.</p><h2 id="autograd">Autograd</h2><p>The ideas behind Automatic Differentiation are quite mature. It can be done in runtime or during compilation, which can have a dramatic impact on performance. I recommend the <a href="https://github.com/HIPS/autograd">HIPS autograd</a> Python package for a thorough explanation of some of the concepts. </p><p>The core idea, however, is always the same, and we have known it ever since we started computing derivatives in school. If we can keep track of the computations that resulted in the final scalar output and we know how to differentiate each simple operation (sums and products, powers, exponentials, and logarithms, ...), we can recover the gradient of the output.</p><p>Let's say that we have some intermediate linear layer $f$ which is characterized by a matrix multiplication (let's leave the bias out for a sec):</p><p>$$y = f(x, w) = x\cdot w$$</p><p>In order to tune the values of $w$ with gradient descent, we need to compute the gradient $\frac{\partial l}{\partial w}$. The key observation that it's enough to know how changes in $y$ will affect $l$ to do that.</p><blockquote>The contract that any layer has to satisfy is the following: if I tell you the gradient of the loss with respect to your outputs, you can tell me the gradient with respect to your inputs, which are in turn the previous layer outputs.</blockquote><p>Now we can apply the chain rule twice: for the gradient with respect to $w$ we get that </p><p>$$\frac{\partial l}{\partial w} =\frac{\partial l}{\partial y}\cdot \frac{\partial y}{\partial w} = \frac{\partial l}{\partial y} \cdot x^t $$</p><p>and with respect to $x$</p><p>$$\frac{\partial l}{\partial x} =\frac{\partial l}{\partial y}\cdot \frac{\partial y}{\partial x} = \frac{\partial l}{\partial y} \cdot w $$ </p><p>So we can both pass a gradient backward to enable previous layers to update themselves and update the internal layer weights to optimize the loss, and that's it!</p><h2 id="implementation-time">Implementation time</h2><p>Let's look at the code, or try it live in this <a href="https://colab.research.google.com/github/eisenjulian/slides/blob/master/NN_from_scratch/notebook.ipynb">Colab Notebook</a>. We can start with a class that encapsulates a tensor and its gradients</p><!--kg-card-begin: markdown--><pre><code class="language-python">class Parameter():
  def __init__(self, tensor):
    self.tensor = tensor
    self.gradient = np.zeros_like(self.tensor)
</code></pre>
<!--kg-card-end: markdown--><p>Now we can create the layer class, the key idea is that during a forward pass we return both the layer output and function that can receive the gradient of the loss with respect to the outputs and can return the gradient with respect to the inputs, updating the weight gradients in the process.</p><p>This is because while evaluating the model layer by layer there's no way to calculate the gradients if we don't know the final loss yet, instead the best thing you can do is return a function that CAN calculate the gradient later. And that function will only be called after we completed the forward evaluation, when you know the loss and you have all the necessary info to compute the gradients in that layer.</p><p>The training process will then have three steps, calculate the forward step, then the backward steps accumulate the gradients, and finally updating the weights. It’s important to do this at the end	 since weights can be reused in multiple layers and we don’t want to mutate the weights before time.</p><!--kg-card-begin: markdown--><pre><code class="language-python">class Layer:
  def __init__(self):
    self.parameters = []

  def forward(self, X):
    &quot;&quot;&quot;
    Override me! A simple no-op layer, it passes forward the inputs
    &quot;&quot;&quot;
    return X, lambda D: D

  def build_param(self, tensor):
    &quot;&quot;&quot;
    Creates a parameter from a tensor, and saves a reference for the update step
    &quot;&quot;&quot;
    param = Parameter(tensor)
    self.parameters.append(param)
    return param

  def update(self, optimizer):
    for param in self.parameters: optimizer.update(param)
</code></pre>
<!--kg-card-end: markdown--><p>It's standard to delegate the job of updating the parameters to an optimizer, which receives an instance of a parameter after every batch. The simplest and most known optimization method out there is the mini-batch stochastic gradient descent</p><!--kg-card-begin: markdown--><pre><code class="language-python">class SGDOptimizer():
  def __init__(self, lr=0.1):
    self.lr = lr

  def update(self, param):
    param.tensor -= self.lr * param.gradient
    param.gradient.fill(0)
</code></pre>
<!--kg-card-end: markdown--><p>Under this framework, and using the results we computed earlier, the linear layer looks like this snippet of code. We are using <code>numpy</code> overloaded <code>@</code> operator for matrix multiplication.</p><!--kg-card-begin: markdown--><pre><code class="language-python">class Linear(Layer):
  def __init__(self, inputs, outputs):
    super().__init__()
    tensor = np.random.randn(inputs, outputs) * np.sqrt(1 / inputs)
    self.weights = self.build_param(tensor)
    self.bias = self.build_param(np.zeros(outputs))

  def forward(self, X):
    def backward(D):
      self.weights.gradient += X.T @ D
      self.bias.gradient += D.sum(axis=0)
      return D @ self.weights.tensor.T
    return X @ self.weights.tensor +  self.bias.tensor, backward
</code></pre>
<!--kg-card-end: markdown--><p>Now, the next most used types of layers are activations, which are non-linear point-wise functions. The Jacobian of a point-wise function is diagonal, which means that when multiplied by the gradient it also acts a point-wise multiplication. </p><!--kg-card-begin: markdown--><pre><code class="language-python">class ReLu(Layer):
  def forward(self, X):
    mask = X &gt; 0
    return X * mask, lambda D: D * mask
</code></pre>
<!--kg-card-end: markdown--><p>A <a href="https://math.stackexchange.com/a/78578/325086">little harder</a> is to compute the derivative of a Sigmoid, which is again applied pointwise:</p><!--kg-card-begin: markdown--><pre><code class="language-python">class Sigmoid(Layer):
  def forward(self, X):
    S = 1 / (1 + np.exp(-X))
    def backward(D):
      return D * S * (1 - S)
    return S, backward
</code></pre>
<!--kg-card-end: markdown--><p>When we have many layers in a sequence, as we traverse them and get the successive outputs we can save the <code>backward</code> functions in a list that will be used in the reverse order, to get all the way to gradient with respect to the first layer inputs and return it. This is where the magic happens:</p><!--kg-card-begin: markdown--><pre><code class="language-python">class Sequential(Layer):
  def __init__(self, *layers):
    super().__init__()
    self.layers = layers
    for layer in layers:
      self.parameters.extend(layer.parameters)

  def forward(self, X):
    backprops = []
    Y = X
    for layer in self.layers:
      Y, backprop = layer.forward(Y)
      backprops.append(backprop)
    def backward(D):
      for backprop in reversed(backprops): 
        D = backprop(D)
      return D
    return Y, backward
</code></pre>
<!--kg-card-end: markdown--><p>As we mentioned earlier, we will need a way to calculate the loss function associated with a batch of samples, and its gradient. One example would be <em>MSE</em> loss, typically used in regression problems, and it can be implemented in this manner:</p><!--kg-card-begin: markdown--><pre><code class="language-python">def mse_loss(Yp, Yt):
  diff = Yp - Yt
  return np.square(diff).mean(), 2 * diff / len(diff)
</code></pre>
<!--kg-card-end: markdown--><p>Almost there now, we have two types of layers and a way to combine them, so how does the training loop look like. We can use an API similar to <code>scikit-learn</code> or <code>keras</code>.</p><!--kg-card-begin: markdown--><pre><code class="language-python">class Learner():
  def __init__(self, model, loss, optimizer):
    self.model = model
    self.loss = loss
    self.optimizer = optimizer

  def fit_batch(self, X, Y):
    Y_, backward = self.model.forward(X)
    L, D = self.loss(Y_, Y)
    backward(D)
    self.model.update(self.optimizer)
    return L

  def fit(self, X, Y, epochs, bs):
    losses = []
    for epoch in range(epochs):
      p = np.random.permutation(len(X))
      X, Y = X[p], Y[p]
      loss = 0.0
      for i in range(0, len(X), bs):
        loss += self.fit_batch(X[i:i + bs], Y[i:i + bs])
      losses.append(loss)
    return losses
</code></pre>
<!--kg-card-end: markdown--><p>And that is all, if you were keeping track, we even have a few lines of code to spare. </p><h2 id="but-does-it-work">But does it work?</h2><p>Thought you would never ask, we can start testing with some synthetic data.</p><!--kg-card-begin: markdown--><pre><code class="language-python">X = np.random.randn(100, 10)
w = np.random.randn(10, 1)
b = np.random.randn(1)
Y = X @ W + B

model = Linear(10, 1)
learner = Learner(model, mse_loss, SGDOptimizer(lr=0.05))
learner.fit(X, Y, epochs=10, bs=10)
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://eisenjulian.github.io/content/images/2019/03/linear_loss.png" class="kg-image" alt="Neural Networks in 100 lines of pure Python"><figcaption>Training loss in 10 epochs</figcaption></figure><p>We can also check that the learned weights coincide with the true ones</p><!--kg-card-begin: markdown--><pre><code class="language-python">print(np.linalg.norm(m.weights.tensor - W), (m.bias.tensor - B)[0])
&gt; 1.848553648022619e-05 5.69305886743976e-06
</code></pre>
<!--kg-card-end: markdown--><p>OK, so that was easy, but let's come up with a non-linear dataset, how about $y = x_1 x_2$, we will add a Sigmoid non-linearity and another linear layer to maker our model more expressive. Here we go:</p><!--kg-card-begin: markdown--><pre><code class="language-python">X = np.random.randn(1000, 2)
Y = X[:, 0] * X[:, 1]

losses1 = Learner(
    Sequential(Linear(2, 1)), 
    mse_loss, 
    SGDOptimizer(lr=0.01)
).fit(X, Y, epochs=50, bs=50)

losses2 = Learner(
    Sequential(
        Linear(2, 10), 
        Sigmoid(), 
        Linear(10, 1)
    ), 
    mse_loss, 
    SGDOptimizer(lr=0.3)
).fit(X, Y, epochs=50, bs=50)

plt.plot(losses1)
plt.plot(losses2)
plt.legend(['1 Layer', '2 Layers'])
plt.show()
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://eisenjulian.github.io/content/images/2019/03/non-linear.png" class="kg-image" alt="Neural Networks in 100 lines of pure Python"><figcaption>Training loss comparing a one layer model vs. a two layer model with a sigmoid activation</figcaption></figure><h2 id="wrapping-up">Wrapping Up</h2><p>I hope you have found this educative, we only defined three types of layers and one loss function, so there's much more to be done. In a follow-up post, we will implement binary cross entropy loss as well as other non-linear activations to start building more expressive models. Stay tuned...</p><p>Reach out on Twitter at <a>@eisenjulian</a> for questions and requests. Thanks for reading!</p><h2 id="references">References</h2><p>[1] Thinc Deep Learning Library <a href="https://github.com/explosion/thinc">https://github.com/explosion/thinc</a><br>[2] PyTorch Tutorial <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">https://pytorch.org/tutorials/beginner/nn_tutorial.html</a><br>[3] Calculus on Computational Graphs <a href="http://colah.github.io/posts/2015-08-Backprop/">http://colah.github.io/posts/2015-08-Backprop/</a><br>[4] HIPS Autograd <a href="https://github.com/HIPS/autograd">https://github.com/HIPS/autograd</a><br></p>]]></content:encoded></item><item><title><![CDATA[Text Classification with TensorFlow Estimators]]></title><description><![CDATA[Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow. ]]></description><link>https://eisenjulian.github.io/text-classification-with-estimators/</link><guid isPermaLink="false">5c8f230f6d2ff81194b67a4f</guid><category><![CDATA[Natural Language Processing]]></category><category><![CDATA[TensorFlow]]></category><category><![CDATA[Tutorials]]></category><dc:creator><![CDATA[Julian Eisenschlos]]></dc:creator><pubDate>Wed, 07 Mar 2018 04:50:00 GMT</pubDate><media:content url="https://eisenjulian.github.io/content/images/2019/03/estimators_loss.png" medium="image"/><content:encoded><![CDATA[<img src="https://eisenjulian.github.io/content/images/2019/03/estimators_loss.png" alt="Text Classification with TensorFlow Estimators"><p>Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow. </p><p><em>Posted by Sebastian Ruder and Julian Eisenschlos, Google Developer Experts</em></p><p>Here’s the outline of what we’ll cover:</p><ul><li>Loading data using Datasets.</li><li>Building baselines using pre-canned estimators.</li><li>Using word embeddings.</li><li>Building custom estimators with convolution and LSTM layers.</li><li>Loading pre-trained word vectors.</li><li>Evaluating and comparing models using TensorBoard.</li></ul><p>Welcome to Part 4 of a blog series that introduces TensorFlow Datasets and Estimators. You don’t need to read all of the previous material, but take a look if you want to refresh any of the following concepts. <a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html" rel="nofollow noopener">Part 1</a> focused on pre-made Estimators, <a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html" rel="nofollow noopener">Part 2</a> discussed feature columns, and <a href="https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html" rel="nofollow noopener">Part 3</a> how to create custom Estimators.</p><p>Here in Part 4, we will build on top of all the above to tackle a different family of problems in Natural Language Processing (NLP). In particular, this article demonstrates how to solve a text classification task using custom TensorFlow estimators, embeddings, and the <a href="https://www.tensorflow.org/api_docs/python/tf/layers" rel="nofollow noopener">tf.layers</a> module. Along the way, we’ll learn about word2vec and transfer learning as a technique to bootstrap model performance when labeled data is a scarce resource.</p><p>We will show you relevant code snippets. <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb" rel="nofollow noopener">Here</a>’s the complete Jupyter Notebook that you can run locally or on <a href="https://goo.gl/fXsCra" rel="nofollow noopener">Google Colaboratory</a>. The plain <code>.py</code> source file is also available <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py" rel="nofollow noopener">here</a>. Note that the code was written to demonstrate how Estimators work functionally and was not optimized for maximum performance.</p><h3 id="the-task">The Task</h3><p>The dataset we will be using is the IMDB <a href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="nofollow noopener">Large Movie Review Dataset</a>, which consists of 25,000 highly polar movie reviews for training, and 25,000 for testing. We will use this dataset to train a binary classification model, able to predict whether a review is positive or negative.</p><p>For illustration, here’s a piece of a negative review (with 2 stars) in the dataset:</p><blockquote><em><em><em>Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film.</em></em></em></blockquote><p><em>Keras</em> provides a convenient handler for importing the dataset which is also available as a serialized numpy array <code>.npz</code> file to download <a href="https://s3.amazonaws.com/text-datasets/imdb.npz" rel="nofollow noopener">here</a>. For text classification, it is standard to limit the size of the vocabulary to prevent the dataset from becoming too sparse and high dimensional, causing potential overfitting. For this reason, each review consists of a series of word indexes that go from 4 (the most frequent word in the dataset: <strong><strong>the</strong></strong>) to 4999, which corresponds to <strong><strong>orange</strong></strong>. Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown (also known as <em>out-of-vocabulary</em> or <em>OOV</em>) tokens. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency.</p><p>After we’ve loaded the data in memory we pad each of the sentences with 0 to a fixed size (here: 200) so that we have two $2$-dimensional $25000\times 200$ arrays for training and testing respectively.</p><!--kg-card-begin: markdown--><pre><code class="language-python">vocab_size = 5000
sentence_size = 200
(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(num_words=vocab_size)
x_train = sequence.pad_sequences(
    x_train_variable, 
    maxlen=sentence_size, 
    padding='post', 
    value=0)
x_test = sequence.pad_sequences(
    x_test_variable,
    maxlen=sentence_size, 
    padding='post', 
    value=0)
</code></pre>
<!--kg-card-end: markdown--><h3 id="input-functions">Input Functions</h3><p>The Estimator framework uses <em>input functions</em> to split the data pipeline from the model itself. Several helper methods are available to create them, whether your data is in a <code>.csv</code> file, or in a <code>pandas.DataFrame</code>, whether it fits in memory or not. In our case, we can use <code>Dataset.from_tensor_slices</code> for both the train and test sets.</p><!--kg-card-begin: markdown--><pre><code class="language-python">x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])
x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])

def parser(x, length, y):
    features = {&quot;x&quot;: x, &quot;len&quot;: length}
    return features, y

def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))
    dataset = dataset.shuffle(buffer_size=len(x_train_variable))
    dataset = dataset.batch(100)
    dataset = dataset.map(parser)
    dataset = dataset.repeat()
    iterator = dataset.make_one_shot_iterator()
    return iterator.get_next()

def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))
    dataset = dataset.batch(100)
    dataset = dataset.map(parser)
    iterator = dataset.make_one_shot_iterator()
    return iterator.get_next()
</code></pre>
<!--kg-card-end: markdown--><p>We shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional <code>"len"</code> key that captures the length of the original, unpadded sequence, which we will use later.</p><p>Datasets can work with out-of-memory sources (not needed in this case) by streaming them record by record, and the <code>shuffle</code> method uses a <code>buffer_size</code> to continuously sample from fixed sized set without loading the entire thing into memory.</p><h3 id="building-a-baseline">Building a baseline</h3><p>It’s good practice to start any machine learning project trying basic baselines. The simpler the better as having a simple and robust baseline is key to understanding exactly how much we are gaining in terms of performance by adding extra complexity. It may very well be the case that a simple solution is good enough for our requirements.</p><p>With that in mind, let us start by trying out one of the simplest models for text classification. That would be a sparse linear model that gives a weight to each token and adds up all of the results, regardless of the order. As this model does not care about the order of words in a sentence, we normally refer to it as a <em>Bag-of-Words</em> approach. Let’s see how we can implement this model using an <code>Estimator</code>.</p><p>We start out by defining the feature column that is used as input to our classifier. As we have seen in <a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html" rel="nofollow noopener">Part 2</a>, <code>categorical_column_with_identity</code> is the right choice for this pre-processed text input. If we were feeding raw text tokens other <code>feature_columns</code> could do a lot of the pre-processing for us. We can now use the pre-made <code>LinearClassifier</code>.</p><!--kg-card-begin: markdown--><pre><code class="language-python">column = tf.feature_column.categorical_column_with_identity('x', vocab_size)
classifier = tf.estimator.LinearClassifier(
    feature_columns=[column], 
    model_dir=os.path.join(model_dir, 'bow_sparse'))
</code></pre>
<!--kg-card-end: markdown--><p>Finally, we create a simple function that trains the classifier and additionally creates a precision-recall curve. As we do not aim to maximize performance in this blog post, we only train our models for $25,000$ steps.</p><!--kg-card-begin: markdown--><pre><code class="language-python">def train_and_evaluate(classifier):
    classifier.train(input_fn=train_input_fn, steps=25000)
    eval_results = classifier.evaluate(input_fn=eval_input_fn)
    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])
    tf.reset_default_graph() 
    # Add a PR summary in addition to the summaries that the classifier writes
    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)
    with tf.Session() as sess:
        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)
        writer.add_summary(sess.run(pr), global_step=0)
        writer.close()
        
train_and_evaluate(classifier)
</code></pre>
<!--kg-card-end: markdown--><p>One of the benefits of choosing a simple model is that it is much more interpretable. The more complex a model, the harder it is to inspect and the more it tends to work like a black box. In this example, we can load the weights from our model’s last checkpoint and take a look at what tokens correspond to the biggest weights in absolute value. The results look like what we would expect.</p><!--kg-card-begin: markdown--><pre><code class="language-python"># Load the tensor with the model weights
weights = classifier.get_variable_value('linear/linear_model/x/weights').flatten()
# Find biggest weights in absolute value
extremes = np.concatenate((sorted_indexes[-8:], sorted_indexes[:8]))
# word_inverted_index is a dictionary that maps from indexes back to tokens
extreme_weights = sorted(
    [(weights[i], word_inverted_index[i]) for i in extremes])
# Create plot
y_pos = np.arange(len(extreme_weights))
plt.bar(y_pos, [pair[0] for pair in extreme_weights], align='center', alpha=0.5)
plt.xticks(y_pos, [pair[1] for pair in extreme_weights], rotation=45, ha='right')
plt.ylabel('Weight')
plt.title('Most significant tokens') 
plt.show()
</code></pre>
<!--kg-card-end: markdown--><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/0*tCGMsIAQ9PTb9lml.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"></figure><p>As we can see, tokens with the most positive weight such as ‘refreshing’ are clearly associated with positive sentiment, while tokens that have a large negative weight unarguably evoke negative emotions. A simple but powerful modification that one can do to improve this model is weighting the tokens by their <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="nofollow noopener">tf-idf</a> scores.</p><h3 id="embeddings">Embeddings</h3><p>The next step of complexity we can add are word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. While an individual dimension is not meaningful, the low-dimensional space — when learned from a large enough corpus — has been shown to capture relations such as tense, plural, gender, thematic relatedness, and many more. We can add word embeddings by converting our existing feature column into an <code>embedding_column</code>. The representation seen by the model is the mean of the embeddings for each token (see the <code>combiner</code> argument in the <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" rel="nofollow noopener">docs</a>). We can plug in the embedded features into a pre-canned <code>DNNClassifier</code>.</p><p>A note for the keen observer: an <code>embedding_column</code> is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending of the chosen combiner. A direct consequence of this is that it wouldn’t make sense to use an <code>embedding_column</code>directly in a <code>LinearClassifier</code> because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.</p><!--kg-card-begin: markdown--><pre><code class="language-python">embedding_size = 50
word_embedding_column = tf.feature_column.embedding_column(
    column, dimension=embedding_size)
classifier = tf.estimator.DNNClassifier(
    hidden_units=[100],
    feature_columns=[word_embedding_column], 
    model_dir=os.path.join(model_dir, 'bow_embeddings'))
train_and_evaluate(classifier)
</code></pre>
<!--kg-card-end: markdown--><p>We can use TensorBoard to visualize our 50-dimensional word vectors projected into $\mathbb{R}^3$ using <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="nofollow noopener">t-SNE</a>. We expect similar words to be close to each other. This can be a useful way to inspect our model weights and find unexpected behaviors.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/0*lxcE4sPyHteV4lVH.gif" class="kg-image" alt="Text Classification with TensorFlow Estimators"></figure><p>At this point one possible approach would be to go deeper, further adding more fully connected layers and playing around with layer sizes and training functions. However, by doing that we would add extra complexity and ignore important structure in our sentences. Words do not live in a vacuum and meaning is compositional, formed by words and its neighbors.</p><p>Convolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for <a href="https://www.tensorflow.org/tutorials/layers" rel="nofollow noopener">image classification</a>. The intuition is that certain sequences of words, or <em>n-grams</em>, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning.</p><p>The following image shows how a filter matrix <em>F</em> of shape <em>d</em>×<em>m</em> slides across each 3-gram window of tokens to build a new feature map. Afterwards a <em>pooling</em> layer is usually applied to combine adjacent results.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/1*TsW55MIvzHwb-GRA21Q7Zw.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"><figcaption>Source: <a href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9" data-href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank" style="background-color: transparent; color: inherit; text-decoration: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.54); background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,<svg preserveAspectRatio=\&quot;none\&quot; viewBox=\&quot;0 0 1 1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot;><line x1=\&quot;0\&quot; y1=\&quot;0\&quot; x2=\&quot;1\&quot; y2=\&quot;1\&quot; stroke=\&quot;currentColor\&quot; /></svg>&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px);">Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</a> by <strong class="markup--strong markup--figure-strong" style="font-weight: 700;">Severyn</strong> et al.&nbsp;[2015]</figcaption></figure><p>Let us look at the full model architecture. The use of dropout layers is a regularization technique that makes the model less likely to overfit.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*zwj1G4Hem-PX1I54j_9gAw.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"></figure><p>As seen in previous blog posts, the <code>tf.estimator</code> framework provides a high-level API for training machine learning models, defining <code>train()</code>, <code>evaluate()</code> and <code>predict()</code> operations, handling checkpointing, loading, initializing, serving, building the graph and the session out of the box. There is a small family of pre-made estimators, like the ones we used earlier, but it’s most likely that you will need to <a href="https://www.tensorflow.org/extend/estimators" rel="nofollow noopener">build your own</a>.</p><p>Writing a custom estimator means writing a <code>model_fn(features, labels, mode, params)</code> that returns an <code>EstimatorSpec</code>. The first step will be mapping the features into our embedding layer:</p><!--kg-card-begin: markdown--><pre><code class="language-python">input_layer = tf.contrib.layers.embed_sequence(
    features['x'], 
    vocab_size, 
    embedding_size,
    initializer=params['embedding_initializer'])
</code></pre>
<!--kg-card-end: markdown--><p>Then we use <code>tf.layers</code> to process each output sequentially.</p><!--kg-card-begin: markdown--><pre><code class="language-python">training = (mode == tf.estimator.ModeKeys.TRAIN)
dropout_emb = tf.layers.dropout(inputs=input_layer, 
                                rate=0.2, 
                                training=training)
conv = tf.layers.conv1d(
    inputs=dropout_emb,
    filters=32,
    kernel_size=3,
    padding=&quot;same&quot;,
    activation=tf.nn.relu)
pool = tf.reduce_max(input_tensor=conv, axis=1)
hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)
dropout = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)
logits = tf.layers.dense(inputs=dropout_hidden, units=1)
</code></pre>
<!--kg-card-end: markdown--><p>Finally, we will use a <code>Head</code> to simplify the writing of our last part of the <code>model_fn</code>. The head already knows how to compute predictions, loss, train_op, metrics and export outputs, and can be reused across models. This is also used in the pre-made estimators and provides us with the benefit of a uniform evaluation function across all of our models. We will use <code>binary_classification_head</code>, which is a head for single label binary classification that uses <code>sigmoid_cross_entropy_with_logits</code> as the loss function under the hood.</p><!--kg-card-begin: markdown--><pre><code class="language-python">head = tf.contrib.estimator.binary_classification_head()
optimizer = tf.train.AdamOptimizer()    
def _train_op_fn(loss):
    tf.summary.scalar('loss', loss)
    return optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())

return head.create_estimator_spec(
    features=features,
    labels=labels,
    mode=mode,
    logits=logits,
    train_op_fn=_train_op_fn)
</code></pre>
<!--kg-card-end: markdown--><p>Running this model is just as easy as before:</p><!--kg-card-begin: markdown--><pre><code class="language-python">initializer = tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
params = {'embedding_initializer': initializer}
cnn_classifier = tf.estimator.Estimator(model_fn=model_fn,
                                        model_dir=os.path.join(model_dir, 'cnn'),
                                        params=params)
train_and_evaluate(cnn_classifier)
</code></pre>
<!--kg-card-end: markdown--><h3 id="lstm-networks">LSTM Networks</h3><p>Using the <code>Estimator</code> API and the same model <code>head</code>, we can also create a classifier that uses a <em>Long Short-Term Memory</em> (<em>LSTM</em>) cell instead of convolutions. Recurrent models such as this are some of the most successful building blocks for NLP applications. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory.</p><p>One of the drawbacks of recurrent models compared to CNNs is that, because of the nature of recursion, models turn out deeper and more complex, which usually produces slower training time and worse convergence. LSTMs (and RNNs in general) can suffer convergence issues like vanishing or exploding gradients, that said, with sufficient tuning they can obtain state-of-the-art results for many problems. As a rule of thumb CNNs are good at feature extraction, while RNNs excel at tasks that depend on the meaning of the whole sentence, like question answering or machine translation.</p><p>Each cell processes one token embedding at a time and updates its internal state based on a differentiable computation that depends on both the embedding vector <em>x</em> at time <em>t</em>​ and the previous state <em>h</em> at time <em>t−1</em>​. In order to get a better understanding of how LSTMs work, you can refer to Chris Olah’s <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow noopener">blog post</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1000/0*_lp7l6lhNBhETERr.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"><figcaption>Source: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank" style="background-color: transparent; color: inherit; text-decoration: none; -webkit-tap-highlight-color: rgba(0, 0, 0, 0.54); background-repeat: repeat-x; background-image: url(&quot;data:image/svg+xml;utf8,<svg preserveAspectRatio=\&quot;none\&quot; viewBox=\&quot;0 0 1 1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot;><line x1=\&quot;0\&quot; y1=\&quot;0\&quot; x2=\&quot;1\&quot; y2=\&quot;1\&quot; stroke=\&quot;currentColor\&quot; /></svg>&quot;); background-size: 1px 1px; background-position: 0px calc(1em + 1px);">Understanding LSTM Networks</a> by <strong class="markup--strong markup--figure-strong" style="font-weight: 700;">Chris&nbsp;Olah</strong></figcaption></figure><p>The complete LSTM model can be expressed by the following simple flowchart:</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*y3w54qFX7D2P0I4FKHUddQ.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"></figure><p>In the beginning of this post, we padded all documents up to 200 tokens, which is necessary to build a proper tensor. However, when a document contains fewer than 200 words, we don’t want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded. Internally, the model then copies the last state through to the sequence’s end. We can do this by using the <code>"len"</code> feature in our input functions. We can now use the same logic as above and simply replace the convolutional, pooling, and flatten layers with our LSTM cell.</p><!--kg-card-begin: markdown--><pre><code class="language-python">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)
_, final_states = tf.nn.dynamic_rnn(
        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)
logits = tf.layers.dense(inputs=final_states.h, units=1)
</code></pre>
<!--kg-card-end: markdown--><p></p><h3 id="pre-trained-vectors">Pre-trained vectors</h3><p>Most of the models that we have shown before rely on word embeddings as a first layer. So far, we have initialized this embedding layer randomly. However, <a href="https://arxiv.org/abs/1607.01759" rel="nofollow noopener">much</a> <a href="https://arxiv.org/abs/1301.3781" rel="nofollow noopener">previous</a> <a href="https://arxiv.org/abs/1103.0398" rel="nofollow noopener">work</a> has shown that using embeddings pre-trained on a large unlabeled corpus as initialization is beneficial, particularly when training on only a small number of labeled examples. The most popular pre-trained embedding is <a href="https://www.tensorflow.org/tutorials/word2vec" rel="nofollow noopener">word2vec</a>. Leveraging knowledge from unlabeled data via pre-trained embeddings is an instance of <a href="http://ruder.io/transfer-learning/" rel="nofollow noopener"><em>transfer learning</em></a>.</p><p>To this end, we will show you how to use them in an <code>Estimator</code>. We will use the pre-trained vectors from another popular model, <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow noopener">GloVe</a>.</p><!--kg-card-begin: markdown--><pre><code class="language-python">embeddings = {}
with open('glove.6B.50d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.strip().split()
        w = values[0]
        vectors = np.asarray(values[1:], dtype='float32')
        embeddings[w] = vectors
</code></pre>
<!--kg-card-end: markdown--><p>After loading the vectors into memory from a file we store them as a <code>numpy.array</code> using the same indexes as our vocabulary. The created array is of shape <code>(5000, 50)</code>. At every row index, it contains the 50-dimensional vector representing the word at the same index in our vocabulary.</p><!--kg-card-begin: markdown--><pre><code class="language-python">embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, embedding_size))
for w, i in word_index.items():
    v = embeddings.get(w)
    if v is not None and i &lt; vocab_size:
        embedding_matrix[i] = v
</code></pre>
<!--kg-card-end: markdown--><p>Finally, we can use a custom initializer function and pass it in the <code>params</code>object to our <code>cnn_model_fn</code> , without any modifications.</p><!--kg-card-begin: markdown--><pre><code class="language-python">def my_initializer(shape=None, dtype=tf.float32, partition_info=None):
    assert dtype is tf.float32
    return embedding_matrix
params = {'embedding_initializer': my_initializer}
cnn_pretrained_classifier = tf.estimator.Estimator(
    model_fn=cnn_model_fn,
    model_dir=os.path.join(model_dir, 'cnn_pretrained'),
    params=params)
train_and_evaluate(cnn_pretrained_classifier)
</code></pre>
<!--kg-card-end: markdown--><h3 id="running-tensorboard">Running TensorBoard</h3><p>Now we can launch TensorBoard and see how the different models we’ve trained compare against each other in terms of training time and performance.</p><p>In a terminal, we run</p><!--kg-card-begin: markdown--><p><code>&gt; tensorboard --logdir={model_dir}</code></p>
<!--kg-card-end: markdown--><p>We can visualize many metrics collected while training and testing, including the loss function values of each model at each training step, and the precision-recall curves. This is of course most useful to select which model works best for our use-case as well as how to choose classification thresholds.</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1000/1*ACyqHB1zNYZQ3YDTiY7WPg.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/750/1*_tL-oN9IvzDCWyVGZ4LF3A.png" class="kg-image" alt="Text Classification with TensorFlow Estimators"><figcaption>Training loss across steps on the left and Precision-Recall curves on the test data for each of our models on the&nbsp;left</figcaption></figure><h3 id="getting-predictions">Getting Predictions</h3><p>To obtain predictions on new sentences we can use the <code>predict</code> method in the <code>Estimator</code> instances, which will load the latest checkpoint for each model and evaluate on the unseen examples. But before passing the data into the model we have to clean up, tokenize and map each token to the corresponding index as we see below.</p><!--kg-card-begin: markdown--><pre><code class="language-python">def text_to_index(sentence):
    # Remove punctuation characters except for the apostrophe
    translator = str.maketrans('', '', string.punctuation.replace(&quot;'&quot;, ''))
    tokens = sentence.translate(translator).lower().split()
    return np.array([1] + [word_index[t] if t in word_index else 2 for t in tokens])

def print_predictions(sentences, classifier):
    indexes = [text_to_index(sentence) for sentence in sentences]
    x = sequence.pad_sequences(indexes, 
                               maxlen=sentence_size, 
                               padding='post', 
                               value=0)
    length = np.array([min(len(x), sentence_size) for x in indexes])
    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={&quot;x&quot;: x, &quot;len&quot;: length}, shuffle=False)
    predictions = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]
    print(predictions)
</code></pre>
<!--kg-card-end: markdown--><p>It is worth noting that the checkpoint itself is not sufficient to make predictions; the actual code used to build the estimator is necessary as well in order to map the saved weights to the corresponding tensors. It’s a good practice to associate saved checkpoints with the branch of code with which they were created.</p><p>If you are interested in exporting the models to disk in a fully recoverable way, you might want to look into the <a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators" rel="nofollow noopener">SavedModel</a> class, which is especially useful for serving your model through an API using <a href="https://github.com/tensorflow/serving" rel="nofollow noopener">TensorFlow Serving</a> or loading it in the browser with <a href="https://js.tensorflow.org/" rel="nofollow noopener">TensorFlow.js</a>.</p><p>In this blog post, we explored how to use estimators for text classification, in particular for the IMDB Reviews Dataset. We trained and visualized our own embeddings, as well as loaded pre-trained ones. We started from a simple baseline and made our way to convolutional neural networks and LSTMs.</p><p>For more details, be sure to check out:</p><ul><li>A <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb" rel="nofollow noopener">Jupyter Notebook</a> that can run locally, or on <a href="https://goo.gl/fXsCra" rel="nofollow noopener">Colaboratory</a>.</li><li>The complete <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py" rel="nofollow noopener">source code</a> for this blog post.</li><li>The TensorFlow <a href="https://www.tensorflow.org/programmers_guide/embedding" rel="nofollow noopener">Embedding</a> guide.</li><li>The TensorFlow <a href="https://www.tensorflow.org/tutorials/word2vec" rel="nofollow noopener">Vector Representation of Words</a> tutorial.</li><li>The <em>NLTK</em> <a href="http://www.nltk.org/book/ch03.html" rel="nofollow noopener">Processing Raw Text</a> chapter on how to design langage pipelines.</li></ul><hr><p>Thanks for reading! If you like you can find us online at <a href="http://ruder.io/" rel="nofollow noopener nofollow noopener nofollow noopener">ruder.io</a> and <a href="https://twitter.com/eisenjulian" rel="nofollow noopener nofollow noopener">@eisenjulian</a>. Send our way all your feedback and questions.</p>]]></content:encoded></item></channel></rss>