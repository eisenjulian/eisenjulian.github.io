<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>Efficient multi-lingual language model fine-tuning</title>

    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Julian Eisenschlos" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Efficient multi-lingual language model fine-tuning" />
    <meta property="og:description" content="Our latest paper studies multilingual text classification and introduces MultiFiT, a novel method based on ULMFiT. MultiFiT, trained on 100 labeled documents in the target language, outperforms multi-lingual BERT, and the LASER algorithm—even though LASER requires a corpus of parallel texts." />
    <meta property="og:url" content="https://eisenjulian.github.io/multifit/" />
    <meta property="og:image" content="https://eisenjulian.github.io/content/images/2020/08/multifit_bootstrapping.png" />
    <meta property="article:published_time" content="2019-09-10T00:00:00.000Z" />
    <meta property="article:modified_time" content="2020-08-22T22:16:54.000Z" />
    <meta property="article:tag" content="Natural Language Processing" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Efficient multi-lingual language model fine-tuning" />
    <meta name="twitter:description" content="Our latest paper studies multilingual text classification and introduces MultiFiT, a novel method based on ULMFiT. MultiFiT, trained on 100 labeled documents in the target language, outperforms multi-lingual BERT, and the LASER algorithm—even though LASER requires a corpus of parallel texts." />
    <meta name="twitter:url" content="https://eisenjulian.github.io/multifit/" />
    <meta name="twitter:image" content="https://eisenjulian.github.io/content/images/2020/08/multifit_bootstrapping.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Julian Eisenschlos" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Natural Language Processing" />
    <meta name="twitter:site" content="@eisenjulian" />
    <meta property="og:image:width" content="2244" />
    <meta property="og:image:height" content="1482" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Julian Eisenschlos",
        "url": "https://eisenjulian.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Julian Eisenschlos",
        "image": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/content/images/2019/03/perfil-square.jpeg",
            "width": 710,
            "height": 710
        },
        "url": "https://eisenjulian.github.io/author/julian/",
        "sameAs": []
    },
    "headline": "Efficient multi-lingual language model fine-tuning",
    "url": "https://eisenjulian.github.io/multifit/",
    "datePublished": "2019-09-10T00:00:00.000Z",
    "dateModified": "2020-08-22T22:16:54.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://eisenjulian.github.io/content/images/2020/08/multifit_bootstrapping.png",
        "width": 2244,
        "height": 1482
    },
    "keywords": "Natural Language Processing",
    "description": "Our latest paper studies multilingual text classification and introduces MultiFiT, a novel method based on ULMFiT. MultiFiT, trained on 100 labeled documents in the target language, outperforms multi-lingual BERT, and the LASER algorithm—even though LASER requires a corpus of parallel texts.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://eisenjulian.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.7" />
    <link rel="alternate" type="application/rss+xml" title="Julian Eisenschlos" href="../../rss/index.html" />

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: var(--ghost-accent-color, #1292EE);
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: var(--ghost-accent-color, #1292EE);
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #15171a;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }

    :root {--ghost-accent-color: #15171A;}
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                Julian Eisenschlos
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Efficient multi-lingual language model fine-tuning</h1>
                <section class="post-meta">
                    Julian Eisenschlos -
                    <time class="post-date" datetime="2019-09-10">10 Sep 2019</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://eisenjulian.github.io/content/images/2020/08/multifit_bootstrapping.png" width="600" height="340" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p><em>Most of the world’s text is not in English. To enable researchers and practitioners to build impactful solutions in their domains, understanding how our NLP architectures fare in many languages needs to be more than an afterthought. In this post, we introduce our latest paper that studies multilingual text classification and introduces MultiFiT, a novel method based on <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a>. MultiFiT, trained on 100 labeled documents in the target language, outperforms <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multi-lingual BERT</a>. It also outperforms the cutting-edge <a href="https://engineering.fb.com/ai-research/laser-multilingual-sentence-embeddings/">LASER</a> algorithm—even though LASER requires a corpus of parallel texts, and MultiFiT does not.</em></p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://nlp.fast.ai/classification/2019/09/10/multifit.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Efficient multi-lingual language model fine-tuning · fast.ai NLP</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><amp-img class="kg-bookmark-icon" src="https://nlp.fast.ai/favicon.png" width="144" height="144" layout="fixed"></amp-img></div></div><div class="kg-bookmark-thumbnail"><amp-img src="https://nlp.fast.ai/images/multifit_vocabularies.png" width="2730" height="1232" layout="responsive"></amp-img></div></a><figcaption>Cross posted from the fast AI blog</figcaption></figure><hr></hr><p><em>This is joint work by Sebastian Ruder, Piotr Czapla, Marcin Kardas, Sylvain Gugger, Jeremy Howard, and Julian Eisenschlos and benefits from the <a href="https://forums.fast.ai/t/language-model-zoo-gorilla/14623">hundreds</a> of <a href="https://forums.fast.ai/t/multilingual-ulmfit/28117">insights</a> into multilingual transfer learning from the whole <a href="https://forums.fast.ai/">fast.ai forum community</a>. We invite you to read <a href="https://arxiv.org/abs/1909.04761">the full EMNLP 2019 paper</a> or check out the code <a href="https://github.com/n-waves/ulmfit-multilingual">here</a>.</em></p><hr></hr><h2 id="introduction">Introduction</h2><p>If you have ever worked on an NLP task in any language other than English, we feel your pain. The last couple of years have brought <a href="http://ruder.io/nlp-imagenet/">impressive progress</a> in deep learning-based approaches for natural language processing tasks and there’s much to be excited about. However, those advances can be slow to transfer beyond English. In the past, most of academia showed little interest in publishing research or building datasets that go beyond the English language, even though industry applications desperately need language-agnostic techniques. Luckily, thanks to efforts around democratizing access to machine learning and initiatives such as the <a href="https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/">Bender rule</a>, the tides are changing.</p><p>Existing approaches for cross-lingual NLP rely on either:</p><ul><li>Parallel data across languages—that is, a corpus of documents with exactly the same contents, but written in different languages. This is very hard to acquire in a general setting.</li><li>A shared vocabulary—that is, a vocabulary that is common across multiple languages. This approach over-represents languages with a lot of data. For some examples, have a look at <a href="http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html">this blog post</a> An example is <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a>, which is very resource-intensive to train, and can struggle when languages are dissimilar.</li></ul><p>The main appeal of cross-lingual models like multilingual BERT are their <em>zero-shot transfer capabilities</em>: given only labels in a high-resource language such as English, they can transfer to another language without any training data in that language. We argue that many low-resource applications do not provide easy access to training data in a high-resource language. Such applications include disaster response on social media, help desks that deal with community needs or support local business owners, etc. In such settings, it is often easier to collect a few hundred training examples in the low-resource language. The utility of zero-shot approaches in general is quite limited; by definition, if you are applying a model to some language, then you have some documents in that language. So it makes sense to use them to help train your model!</p><p>In addition, when the target language is very different to the source language (most often English), zero-shot transfer may perform poorly or fail altogether. We have seen this with <a href="https://arxiv.org/abs/1805.03620">cross-lingual word embeddings</a> and more recently for <a href="https://arxiv.org/abs/1906.01502">multilingual BERT</a>.</p><p>We show that we can fine-tune efficient monolingual language models that are competitive with multilingual BERT, in many languages, on a few hundred examples. Our proposed approach Multilingual Fine-Tuning (MultiFiT) is different in a number of ways from the current main stream of NLP models: We do not build on BERT, but leverage a more efficient variant of an LSTM architecture. Consequently, our approach is much cheaper to pretrain and more efficient in terms of space and time complexity. Lastly, we emphasize having nimble monolingual models vs. a monolithic cross-lingual one. We also show that we can achieve superior zero-shot transfer by using a cross-lingual model as the teacher. This highlights the potential of combining monolingual and cross-lingual information.</p><h2 id="our-approach">Our approach</h2><p>Our method is based on <a href="https://arxiv.org/abs/1801.06146">Universal Language Model Fine-Tuning (ULMFiT)</a>. For more context, we invite you to check out the <a href="https://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html">previous blog post</a> that explains it in depth. MultiFiT extends ULMFiT to make it more efficient and more suitable for language modelling beyond English: It utilizes tokenization based on subwords rather than words and employs a QRNN rather than an LSTM. In addition, it leverages a number of other improvements.</p><p><strong>Subword tokenization</strong>   ULMFiT uses word-based tokenization, which works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish. Some languages such as Chinese don’t really even have the concept of a “word”, so require heuristic segmentation approaches, which tend to be complicated, slow, and inaccurate. On the other extreme as can be seen below, character-based models use individual characters as tokens. While in this case the vocabulary (and thus the number of parameters) can be small, such models require modelling longer dependencies and can thus be harder to train and less expressive than word-based models.</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://nlp.fast.ai/images/multifit_vocabularies.png" class="kg-image" alt="From character-based to word-based tokenization." width="2730" height="1232" layout="responsive"></amp-img><figcaption>From character-based to word-based tokenization.</figcaption></figure><p>To mitigate this, similar to current neural machine translation models and pretrained language models like BERT and GPT-2, we employ <a href="https://www.aclweb.org/anthology/D18-2012.pdf">SentencePiece subword tokenization</a>, which has since been incorporated into the <a href="https://docs.fast.ai/text.data.html#SPProcessor">fast.ai text package</a>. Subword tokenization strikes a balance between the two approaches by using a mixture of character, subword and word tokens, depending on how common they are.</p><p>This way we can have short (on average) representations of sentences, yet are still able to encode rare words. We use a unigram language model based on Wikipedia that learns a vocabulary of tokens together with their probability of occurrence. It assumes that tokens occur independently (hence the unigram in the name). During tokenization this method finds the most probable segmentation into tokens from the vocabulary. In the image below we show an example of tokenizing “_subwords” using a vocabulary trained on English Wikipedia (“_” is used by SentencePiece to denote a whitespace).</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://nlp.fast.ai/images/multifit_bpe_tokenization_path.png" class="kg-image" alt="A graph of possible subword tokenizations for the token '_subwords'. The number next to each token is its negative log 
likelihood. The most probable tokenization corresponds to the shortest weighted path connecting the blue nodes 
(indicated in red)." width="2796" height="754" layout="responsive"></amp-img><figcaption>A graph of possible subword tokenizations for the token '_subwords'. The number next to each token is its negative log likelihood. The most probable tokenization corresponds to the shortest weighted path connecting the blue nodes (indicated in red).</figcaption></figure><p>To sum up, subword tokenization has two very desirable properties for multilingual language modelling:</p><ol><li>Subwords more easily represent inflections, including common prefixes and suffixes and are thus well-suited for morphologically rich languages.</li><li>Subword tokenization is a good fit for open-vocabulary problems and eliminates out-of-vocabulary tokens, as the coverage is close to 100% tokens.</li></ol><p><strong>QRNN</strong> ULMFiT used a state-of-the-art language model at the time, the <a href="https://arxiv.org/abs/1708.02182">AWD-LSTM</a>. The AWD-LSTM is a regular LSTM with tuned dropout hyper-parameters. While recent state-of-the-art language models have been increasingly based on Transformers, such as the <a href="https://arxiv.org/abs/1901.02860">Transformer-XL</a>, <a href="https://arxiv.org/abs/1909.01792">recurrent models still seem to have the edge on smaller datasets</a> such as the Penn Treebank and WikiText-2.</p><p>To make our model more efficient, we replace the AWD-LSTM with a <a href="https://arxiv.org/abs/1611.01576">Quasi-Recurrent Neural Network (QRNN)</a>. The QRNN strikes a balance between an CNN and an LSTM: It can be parallelized across time and minibatch dimensions like a CNN and inherits the LSTM’s sequential bias as the output depends on the order of elements in the sequence. Specifically, the QRNN alternates convolutional layers, which are parallel across timesteps and a recurrent pooling function, which is parallel across channels.</p><p>We can see in the figure below how it differs from an LSTM and a CNN. In the LSTM, computation at each timestep depends on the results from the previous timestep (indicated by the non-continuous blocks), while CNNs and QRNNs are more easily parallelizable (indicated by the continuous blocks).</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://nlp.fast.ai/images/multifit_qrnn.png" class="kg-image" alt="The computation structure of the QRNN compared with an LSTM and a CNN (Bradbury et al., 2019)" width="1686" height="362" layout="responsive"></amp-img><figcaption>The computation structure of the QRNN compared with an LSTM and a CNN (Bradbury et al., 2019)</figcaption></figure><p>In our experiments, we obtain a 2-3x speed-up during training using QRNNs. QRNNs have been used in a number of applications, such as <a href="https://arxiv.org/abs/1705.08947">state-of-the-art speech recognition</a> in the past.</p><p><strong>Other improvements</strong> Instead of using ULMFiT’s slanted triangular learning rate schedule and gradual unfreezing, we achieve faster training and convergence by employing a cosine variant of the <a href="https://sgugger.github.io/the-1cycle-policy.html">one-cycle policy</a> that is available in the <a href="https://docs.fast.ai/callbacks.one_cycle.html">fast.ai library</a>. Finally, we use <a href="https://rickwierenga.com/blog/fast.ai/FastAI2019-12.html">label smoothing</a>, which transforms the one-hot labels to a “smoother” distribution and has been found particularly useful when learning from noisy labels.</p><p>ULMFiT ensembles the predictions of a forward and backward language model. Even though <a href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5a8fba10ed_1_114">bidirectionality has been found to be important</a> in contextual word vectors, we did not see big improvements for our downstream tasks (text classification) with <a href="https://aclweb.org/anthology/N18-1202/">ELMo-style joint training</a>. As joint training is quite memory-intensive and we emphasize efficiency, we opted to just train forward language models for all languages.</p><p>The full model can be seen in the below figure. It consists of a subword embedding layer, four QRNN layers, an aggregation layer, and two linear layers. The aggregation and linear layers are the same as used in ULMFiT.</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://nlp.fast.ai/images/multifit_architecture.png" class="kg-image" alt="The MultiFiT language model with a classifier head. 
The dimensionality of each layer can be seen in each box at the top (Figure 1 in the paper)." width="2820" height="942" layout="responsive"></amp-img><figcaption>The MultiFiT language model with a classifier head. The dimensionality of each layer can be seen in each box at the top (Figure 1 in the paper).</figcaption></figure><h2 id="results">Results</h2><p>We compare our model to state-of-the-art cross-lingual models including <a href="https://github.com/google-research/bert/blob/master/multilingual.md">multilingual BERT</a> and <a href="https://arxiv.org/abs/1812.10464">LASER</a> (which uses parallel sentences) on two multilingual document classification datasets. Perhaps surprisingly, we find that our monolingual language models fine-tuned only on 100 labeled examples of the corresponding task in the target language outperform zero-shot inference (trained on 1000 examples in the source language) with multilingual BERT and LASER. MultiFit also outperforms the other methods when all models are fine-tuned on 1000 target language examples.</p><p>For the detailed results, have a look at <a href="https://arxiv.org/abs/1909.04761">the paper</a>.</p><h2 id="zero-shot-transfer-with-a-cross-lingual-teacher">Zero-shot Transfer with a Cross-lingual Teacher</h2><p>Still, if a powerful cross-lingual model and labeled data in a high-resource language are available, it would be nice to make use of them in some way. To this end, we propose to use the classifier that is learned on top of the cross-lingual model on the source language data as a teacher to obtain labels for training our model on the target language. This way, we can perform zero-shot transfer using our monolingual language model by bootstrapping from a cross-lingual one.</p><p>To illustrate how this works, take a look at the following diagram:</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://nlp.fast.ai/images/multifit_bootstrapping.png" class="kg-image" alt="The steps of the cross-lingual bootstrapping method for zero-shot cross-lingual transfer (Figure 2 in
the paper)." width="2244" height="1482" layout="responsive"></amp-img><figcaption>The steps of the cross-lingual bootstrapping method for zero-shot cross-lingual transfer (Figure 2 in the paper).</figcaption></figure><p>The process consists of three main steps:</p><ol><li>Our monolingual language model is pre-trained on Wikipedia data in the target language (a) and fine-tuned on in-domain data of the corresponding task (b).</li><li>We now train a classifier on top of cross-lingual model such as LASER using labelled data in a high-resource source language and perform zero-shot inference as usual with this classifier to predict labels on target language documents.</li><li>In a the final step (c), we can now use these predicted labels to fine-tune a classifier on top of our fine-tuned monolingual language model.</li></ol><p>This is similar to <a href="https://arxiv.org/abs/1503.02531">distillation</a>, which has recently been used to <a href="https://arxiv.org/abs/1909.00100">train smaller</a> <a href="https://arxiv.org/abs/1910.01108">language models</a> or <a href="https://arxiv.org/abs/1903.12136">distill task-specific information into downstream models</a>. In contrast, to previous work, we do not just seek to distill the information of a big model into a small model but into one with a <em>different inductive bias</em>. In addition to circumventing the need for labels in the target language, our approach thus brings another benefit: As the monolingual model is specialized to the target language, its inductive bias might be more suitable than the more language-agnostic representations learned by the cross-lingual model. It might thus be able to make better use of labels in the target language, even if they are noisy.</p><p>We obtain evidence for this hypothesis as the monolingual language model fine-tuned on zero-shot predictions outperforms its teacher in all settings.</p><h2 id="robustness-to-noise">Robustness to Noise</h2><p>Another hypothesis why this teaching works so well is that pre-training makes the monolingual language robust to noise to some extent. The pre-trained information stored in the model may act as a regularizer, biasing it towards the correct labels that are in line with its knowledge of the language.</p><p>To test this, we compare a pre-trained language model with a non-pre-trained language model that are fine-tuned on 1k or 10k labelled examples where labels are perturbed with a probability ranging from 0 to 0.75 in the below diagram.</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://nlp.fast.ai/images/multifit_noise_comparison.png" class="kg-image" alt="Comparison of MultiFiT's robustness to label noise with and without pretraining. 
The red line shows the theoretical accuracy of a perfect model that achieves 100% accuracy with all labels." width="1170" height="862" layout="responsive"></amp-img><figcaption>Comparison of MultiFiT's robustness to label noise with and without pretraining. The red line shows the theoretical accuracy of a perfect model that achieves 100% accuracy with all labels.</figcaption></figure><p>As we can see, the pre-trained models are much more robust to label noise. Even with 30% noisy labels, they still maintain about the same performance, whereas the performance of the models without pre-training quickly decays. This highlights robustness to noise as an additional benefit of transfer learning and may facilitate faster crowd-sourcing and data annotation.</p><h2 id="next-steps">Next Steps</h2><p>We are initially releasing seven pre-trained language models in German, Spanish, French, Italian, Japanese, Russian, and Chinese, since they are the ones in the datasets we studied. You can find the code <a href="https://github.com/n-waves/ulmfit-multilingual">here</a>. We hope to release many many more, with the help of the community.</p><p>The fast.ai community has been very helpful in collecting datasets in many more languages, and applying MultiFiT to them—nearly always with state-of-the-art results. For space limitations in the paper those datasets were not included, and we opted to select well used, balanced multi-lingual datasets. Special thanks to <a href="https://aayux.github.io/">Aayush Yadav</a>, <a href="https://twitter.com/aDemyan4uk">Alexey Demyanchuk</a>, <a href="https://github.com/benjaminvdb">Benjamin van der Burgh</a>, <a href="https://github.com/cahya-wirawan">Cahya Wirawan</a>, <a href="https://github.com/cstorm125">Charin Polpanumas</a>, <a href="https://twitter.com/NirantK">Nirant Kasliwal</a> and <a href="https://github.com/tpietruszka">Tomasz Pietruszka</a>.</p><p>Another interesting question to explore further is how very low-resource languages or dialects can benefit from larger corpora in similar languages. We are looking forward to seeing what problems you apply MultiFiT to, so don’t hesitate to ask and share your results in the <a href="https://forums.fast.ai/">fast.ai forums</a>.</p>

            </section>

        </article>
    </main>
    <footer class="page-footer">
        <h3>Julian Eisenschlos</h3>
            <p>Machine Learning, Natural Language Processing, Math • AI Research @ Google • Co-founder botmaker.com • Previously Facebook &amp; ASAPP</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>
</html>
