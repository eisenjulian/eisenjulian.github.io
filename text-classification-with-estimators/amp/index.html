<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>Text Classification with TensorFlow Estimators</title>

    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Julian Eisenschlos" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Text Classification with TensorFlow Estimators" />
    <meta property="og:description" content="Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow." />
    <meta property="og:url" content="https://eisenjulian.github.io/text-classification-with-estimators/" />
    <meta property="og:image" content="https://eisenjulian.github.io/content/images/2019/03/estimators_loss.png" />
    <meta property="article:published_time" content="2018-03-07T04:50:00.000Z" />
    <meta property="article:modified_time" content="2019-03-20T12:38:35.000Z" />
    <meta property="article:tag" content="Natural Language Processing" />
    <meta property="article:tag" content="TensorFlow" />
    <meta property="article:tag" content="Tutorials" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Text Classification with TensorFlow Estimators" />
    <meta name="twitter:description" content="Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow." />
    <meta name="twitter:url" content="https://eisenjulian.github.io/text-classification-with-estimators/" />
    <meta name="twitter:image" content="https://eisenjulian.github.io/content/images/2019/03/estimators_loss.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Julian Eisenschlos" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Natural Language Processing, TensorFlow, Tutorials" />
    <meta name="twitter:site" content="@eisenjulian" />
    <meta property="og:image:width" content="1403" />
    <meta property="og:image:height" content="552" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Julian Eisenschlos",
        "url": "https://eisenjulian.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Julian Eisenschlos",
        "image": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/content/images/2019/03/perfil-square.jpeg",
            "width": 710,
            "height": 710
        },
        "url": "https://eisenjulian.github.io/author/julian/",
        "sameAs": []
    },
    "headline": "Text Classification with TensorFlow Estimators",
    "url": "https://eisenjulian.github.io/text-classification-with-estimators/",
    "datePublished": "2018-03-07T04:50:00.000Z",
    "dateModified": "2019-03-20T12:38:35.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://eisenjulian.github.io/content/images/2019/03/estimators_loss.png",
        "width": 1403,
        "height": 552
    },
    "keywords": "Natural Language Processing, TensorFlow, Tutorials",
    "description": "Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow. ",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://eisenjulian.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.30" />
    <link rel="alternate" type="application/rss+xml" title="Julian Eisenschlos" href="../../rss/index.html" />

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: #1292EE;
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: #000;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #dc0050;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-anim" src="https://cdn.ampproject.org/v0/amp-anim-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                Julian Eisenschlos
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Text Classification with TensorFlow Estimators</h1>
                <section class="post-meta">
                    Julian Eisenschlos -
                    <time class="post-date" datetime="2018-03-07">07 Mar 2018</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://eisenjulian.github.io/content/images/2019/03/estimators_loss.png" width="600" height="340" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow. </p><p><em>Posted by Sebastian Ruder and Julian Eisenschlos, Google Developer Experts</em></p><p>Here’s the outline of what we’ll cover:</p><ul><li>Loading data using Datasets.</li><li>Building baselines using pre-canned estimators.</li><li>Using word embeddings.</li><li>Building custom estimators with convolution and LSTM layers.</li><li>Loading pre-trained word vectors.</li><li>Evaluating and comparing models using TensorBoard.</li></ul><p>Welcome to Part 4 of a blog series that introduces TensorFlow Datasets and Estimators. You don’t need to read all of the previous material, but take a look if you want to refresh any of the following concepts. <a href="https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html" rel="nofollow noopener">Part 1</a> focused on pre-made Estimators, <a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html" rel="nofollow noopener">Part 2</a> discussed feature columns, and <a href="https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html" rel="nofollow noopener">Part 3</a> how to create custom Estimators.</p><p>Here in Part 4, we will build on top of all the above to tackle a different family of problems in Natural Language Processing (NLP). In particular, this article demonstrates how to solve a text classification task using custom TensorFlow estimators, embeddings, and the <a href="https://www.tensorflow.org/api_docs/python/tf/layers" rel="nofollow noopener">tf.layers</a> module. Along the way, we’ll learn about word2vec and transfer learning as a technique to bootstrap model performance when labeled data is a scarce resource.</p><p>We will show you relevant code snippets. <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb" rel="nofollow noopener">Here</a>’s the complete Jupyter Notebook that you can run locally or on <a href="https://goo.gl/fXsCra" rel="nofollow noopener">Google Colaboratory</a>. The plain <code>.py</code> source file is also available <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py" rel="nofollow noopener">here</a>. Note that the code was written to demonstrate how Estimators work functionally and was not optimized for maximum performance.</p><h3 id="the-task">The Task</h3><p>The dataset we will be using is the IMDB <a href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="nofollow noopener">Large Movie Review Dataset</a>, which consists of 25,000 highly polar movie reviews for training, and 25,000 for testing. We will use this dataset to train a binary classification model, able to predict whether a review is positive or negative.</p><p>For illustration, here’s a piece of a negative review (with 2 stars) in the dataset:</p><blockquote><em><em><em>Now, I LOVE Italian horror films. The cheesier they are, the better. However, this is not cheesy Italian. This is week-old spaghetti sauce with rotting meatballs. It is amateur hour on every level. There is no suspense, no horror, with just a few drops of blood scattered around to remind you that you are in fact watching a horror film.</em></em></em></blockquote><p><em>Keras</em> provides a convenient handler for importing the dataset which is also available as a serialized numpy array <code>.npz</code> file to download <a href="https://s3.amazonaws.com/text-datasets/imdb.npz" rel="nofollow noopener">here</a>. For text classification, it is standard to limit the size of the vocabulary to prevent the dataset from becoming too sparse and high dimensional, causing potential overfitting. For this reason, each review consists of a series of word indexes that go from 4 (the most frequent word in the dataset: <strong><strong>the</strong></strong>) to 4999, which corresponds to <strong><strong>orange</strong></strong>. Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown (also known as <em>out-of-vocabulary</em> or <em>OOV</em>) tokens. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency.</p><p>After we’ve loaded the data in memory we pad each of the sentences with 0 to a fixed size (here: 200) so that we have two $2$-dimensional $25000\times 200$ arrays for training and testing respectively.</p><pre><code class="language-python">vocab_size = 5000
sentence_size = 200
(x_train_variable, y_train), (x_test_variable, y_test) = imdb.load_data(num_words=vocab_size)
x_train = sequence.pad_sequences(
    x_train_variable, 
    maxlen=sentence_size, 
    padding='post', 
    value=0)
x_test = sequence.pad_sequences(
    x_test_variable,
    maxlen=sentence_size, 
    padding='post', 
    value=0)
</code></pre>
<h3 id="input-functions">Input Functions</h3><p>The Estimator framework uses <em>input functions</em> to split the data pipeline from the model itself. Several helper methods are available to create them, whether your data is in a <code>.csv</code> file, or in a <code>pandas.DataFrame</code>, whether it fits in memory or not. In our case, we can use <code>Dataset.from_tensor_slices</code> for both the train and test sets.</p><pre><code class="language-python">x_len_train = np.array([min(len(x), sentence_size) for x in x_train_variable])
x_len_test = np.array([min(len(x), sentence_size) for x in x_test_variable])

def parser(x, length, y):
    features = {"x": x, "len": length}
    return features, y

def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((x_train, x_len_train, y_train))
    dataset = dataset.shuffle(buffer_size=len(x_train_variable))
    dataset = dataset.batch(100)
    dataset = dataset.map(parser)
    dataset = dataset.repeat()
    iterator = dataset.make_one_shot_iterator()
    return iterator.get_next()

def eval_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((x_test, x_len_test, y_test))
    dataset = dataset.batch(100)
    dataset = dataset.map(parser)
    iterator = dataset.make_one_shot_iterator()
    return iterator.get_next()
</code></pre>
<p>We shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional <code>"len"</code> key that captures the length of the original, unpadded sequence, which we will use later.</p><p>Datasets can work with out-of-memory sources (not needed in this case) by streaming them record by record, and the <code>shuffle</code> method uses a <code>buffer_size</code> to continuously sample from fixed sized set without loading the entire thing into memory.</p><h3 id="building-a-baseline">Building a baseline</h3><p>It’s good practice to start any machine learning project trying basic baselines. The simpler the better as having a simple and robust baseline is key to understanding exactly how much we are gaining in terms of performance by adding extra complexity. It may very well be the case that a simple solution is good enough for our requirements.</p><p>With that in mind, let us start by trying out one of the simplest models for text classification. That would be a sparse linear model that gives a weight to each token and adds up all of the results, regardless of the order. As this model does not care about the order of words in a sentence, we normally refer to it as a <em>Bag-of-Words</em> approach. Let’s see how we can implement this model using an <code>Estimator</code>.</p><p>We start out by defining the feature column that is used as input to our classifier. As we have seen in <a href="https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html" rel="nofollow noopener">Part 2</a>, <code>categorical_column_with_identity</code> is the right choice for this pre-processed text input. If we were feeding raw text tokens other <code>feature_columns</code> could do a lot of the pre-processing for us. We can now use the pre-made <code>LinearClassifier</code>.</p><pre><code class="language-python">column = tf.feature_column.categorical_column_with_identity('x', vocab_size)
classifier = tf.estimator.LinearClassifier(
    feature_columns=[column], 
    model_dir=os.path.join(model_dir, 'bow_sparse'))
</code></pre>
<p>Finally, we create a simple function that trains the classifier and additionally creates a precision-recall curve. As we do not aim to maximize performance in this blog post, we only train our models for $25,000$ steps.</p><pre><code class="language-python">def train_and_evaluate(classifier):
    classifier.train(input_fn=train_input_fn, steps=25000)
    eval_results = classifier.evaluate(input_fn=eval_input_fn)
    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])
    tf.reset_default_graph() 
    # Add a PR summary in addition to the summaries that the classifier writes
    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool), num_thresholds=21)
    with tf.Session() as sess:
        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)
        writer.add_summary(sess.run(pr), global_step=0)
        writer.close()
        
train_and_evaluate(classifier)
</code></pre>
<p>One of the benefits of choosing a simple model is that it is much more interpretable. The more complex a model, the harder it is to inspect and the more it tends to work like a black box. In this example, we can load the weights from our model’s last checkpoint and take a look at what tokens correspond to the biggest weights in absolute value. The results look like what we would expect.</p><pre><code class="language-python"># Load the tensor with the model weights
weights = classifier.get_variable_value('linear/linear_model/x/weights').flatten()
# Find biggest weights in absolute value
extremes = np.concatenate((sorted_indexes[-8:], sorted_indexes[:8]))
# word_inverted_index is a dictionary that maps from indexes back to tokens
extreme_weights = sorted(
    [(weights[i], word_inverted_index[i]) for i in extremes])
# Create plot
y_pos = np.arange(len(extreme_weights))
plt.bar(y_pos, [pair[0] for pair in extreme_weights], align='center', alpha=0.5)
plt.xticks(y_pos, [pair[1] for pair in extreme_weights], rotation=45, ha='right')
plt.ylabel('Weight')
plt.title('Most significant tokens') 
plt.show()
</code></pre>
<figure class="kg-card kg-image-card"><amp-img src="https://cdn-images-1.medium.com/max/1000/0*tCGMsIAQ9PTb9lml.png" class="kg-image" alt width="493" height="390" layout="responsive"></amp-img></figure><p>As we can see, tokens with the most positive weight such as ‘refreshing’ are clearly associated with positive sentiment, while tokens that have a large negative weight unarguably evoke negative emotions. A simple but powerful modification that one can do to improve this model is weighting the tokens by their <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="nofollow noopener">tf-idf</a> scores.</p><h3 id="embeddings">Embeddings</h3><p>The next step of complexity we can add are word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. While an individual dimension is not meaningful, the low-dimensional space — when learned from a large enough corpus — has been shown to capture relations such as tense, plural, gender, thematic relatedness, and many more. We can add word embeddings by converting our existing feature column into an <code>embedding_column</code>. The representation seen by the model is the mean of the embeddings for each token (see the <code>combiner</code> argument in the <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" rel="nofollow noopener">docs</a>). We can plug in the embedded features into a pre-canned <code>DNNClassifier</code>.</p><p>A note for the keen observer: an <code>embedding_column</code> is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending of the chosen combiner. A direct consequence of this is that it wouldn’t make sense to use an <code>embedding_column</code>directly in a <code>LinearClassifier</code> because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.</p><pre><code class="language-python">embedding_size = 50
word_embedding_column = tf.feature_column.embedding_column(
    column, dimension=embedding_size)
classifier = tf.estimator.DNNClassifier(
    hidden_units=[100],
    feature_columns=[word_embedding_column], 
    model_dir=os.path.join(model_dir, 'bow_embeddings'))
train_and_evaluate(classifier)
</code></pre>
<p>We can use TensorBoard to visualize our 50-dimensional word vectors projected into $\mathbb{R}^3$ using <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="nofollow noopener">t-SNE</a>. We expect similar words to be close to each other. This can be a useful way to inspect our model weights and find unexpected behaviors.</p><figure class="kg-card kg-image-card"><amp-anim src="https://cdn-images-1.medium.com/max/1000/0*lxcE4sPyHteV4lVH.gif" class="kg-image" alt width="982" height="638" layout="responsive"></amp-anim></figure><p>At this point one possible approach would be to go deeper, further adding more fully connected layers and playing around with layer sizes and training functions. However, by doing that we would add extra complexity and ignore important structure in our sentences. Words do not live in a vacuum and meaning is compositional, formed by words and its neighbors.</p><p>Convolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for <a href="https://www.tensorflow.org/tutorials/layers" rel="nofollow noopener">image classification</a>. The intuition is that certain sequences of words, or <em>n-grams</em>, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning.</p><p>The following image shows how a filter matrix <em>F</em> of shape <em>d</em>×<em>m</em> slides across each 3-gram window of tokens to build a new feature map. Afterwards a <em>pooling</em> layer is usually applied to combine adjacent results.</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://cdn-images-1.medium.com/max/1000/1*TsW55MIvzHwb-GRA21Q7Zw.png" class="kg-image" alt width="1000" height="588" layout="responsive"></amp-img><figcaption>Source: <a href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9" data-href="https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Convolution-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks</a> by <strong class="markup--strong markup--figure-strong">Severyn</strong> et al. [2015]</figcaption></figure><p>Let us look at the full model architecture. The use of dropout layers is a regularization technique that makes the model less likely to overfit.</p><figure class="kg-card kg-image-card"><amp-img src="https://cdn-images-1.medium.com/max/1000/1*zwj1G4Hem-PX1I54j_9gAw.png" class="kg-image" alt width="783" height="56" layout="responsive"></amp-img></figure><p>As seen in previous blog posts, the <code>tf.estimator</code> framework provides a high-level API for training machine learning models, defining <code>train()</code>, <code>evaluate()</code> and <code>predict()</code> operations, handling checkpointing, loading, initializing, serving, building the graph and the session out of the box. There is a small family of pre-made estimators, like the ones we used earlier, but it’s most likely that you will need to <a href="https://www.tensorflow.org/extend/estimators" rel="nofollow noopener">build your own</a>.</p><p>Writing a custom estimator means writing a <code>model_fn(features, labels, mode, params)</code> that returns an <code>EstimatorSpec</code>. The first step will be mapping the features into our embedding layer:</p><pre><code class="language-python">input_layer = tf.contrib.layers.embed_sequence(
    features['x'], 
    vocab_size, 
    embedding_size,
    initializer=params['embedding_initializer'])
</code></pre>
<p>Then we use <code>tf.layers</code> to process each output sequentially.</p><pre><code class="language-python">training = (mode == tf.estimator.ModeKeys.TRAIN)
dropout_emb = tf.layers.dropout(inputs=input_layer, 
                                rate=0.2, 
                                training=training)
conv = tf.layers.conv1d(
    inputs=dropout_emb,
    filters=32,
    kernel_size=3,
    padding="same",
    activation=tf.nn.relu)
pool = tf.reduce_max(input_tensor=conv, axis=1)
hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)
dropout = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)
logits = tf.layers.dense(inputs=dropout_hidden, units=1)
</code></pre>
<p>Finally, we will use a <code>Head</code> to simplify the writing of our last part of the <code>model_fn</code>. The head already knows how to compute predictions, loss, train_op, metrics and export outputs, and can be reused across models. This is also used in the pre-made estimators and provides us with the benefit of a uniform evaluation function across all of our models. We will use <code>binary_classification_head</code>, which is a head for single label binary classification that uses <code>sigmoid_cross_entropy_with_logits</code> as the loss function under the hood.</p><pre><code class="language-python">head = tf.contrib.estimator.binary_classification_head()
optimizer = tf.train.AdamOptimizer()    
def _train_op_fn(loss):
    tf.summary.scalar('loss', loss)
    return optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())

return head.create_estimator_spec(
    features=features,
    labels=labels,
    mode=mode,
    logits=logits,
    train_op_fn=_train_op_fn)
</code></pre>
<p>Running this model is just as easy as before:</p><pre><code class="language-python">initializer = tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
params = {'embedding_initializer': initializer}
cnn_classifier = tf.estimator.Estimator(model_fn=model_fn,
                                        model_dir=os.path.join(model_dir, 'cnn'),
                                        params=params)
train_and_evaluate(cnn_classifier)
</code></pre>
<h3 id="lstm-networks">LSTM Networks</h3><p>Using the <code>Estimator</code> API and the same model <code>head</code>, we can also create a classifier that uses a <em>Long Short-Term Memory</em> (<em>LSTM</em>) cell instead of convolutions. Recurrent models such as this are some of the most successful building blocks for NLP applications. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory.</p><p>One of the drawbacks of recurrent models compared to CNNs is that, because of the nature of recursion, models turn out deeper and more complex, which usually produces slower training time and worse convergence. LSTMs (and RNNs in general) can suffer convergence issues like vanishing or exploding gradients, that said, with sufficient tuning they can obtain state-of-the-art results for many problems. As a rule of thumb CNNs are good at feature extraction, while RNNs excel at tasks that depend on the meaning of the whole sentence, like question answering or machine translation.</p><p>Each cell processes one token embedding at a time and updates its internal state based on a differentiable computation that depends on both the embedding vector <em>x</em> at time <em>t</em>​ and the previous state <em>h</em> at time <em>t−1</em>​. In order to get a better understanding of how LSTMs work, you can refer to Chris Olah’s <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow noopener">blog post</a>.</p><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://cdn-images-1.medium.com/max/1000/0*_lp7l6lhNBhETERr.png" class="kg-image" alt width="1000" height="375" layout="responsive"></amp-img><figcaption>Source: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" data-href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Understanding LSTM Networks</a> by <strong class="markup--strong markup--figure-strong">Chris Olah</strong></figcaption></figure><p>The complete LSTM model can be expressed by the following simple flowchart:</p><figure class="kg-card kg-image-card"><amp-img src="https://cdn-images-1.medium.com/max/1000/1*y3w54qFX7D2P0I4FKHUddQ.png" class="kg-image" alt width="565" height="185" layout="responsive"></amp-img></figure><p>In the beginning of this post, we padded all documents up to 200 tokens, which is necessary to build a proper tensor. However, when a document contains fewer than 200 words, we don’t want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded. Internally, the model then copies the last state through to the sequence’s end. We can do this by using the <code>"len"</code> feature in our input functions. We can now use the same logic as above and simply replace the convolutional, pooling, and flatten layers with our LSTM cell.</p><pre><code class="language-python">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)
_, final_states = tf.nn.dynamic_rnn(
        lstm_cell, inputs, sequence_length=features['len'], dtype=tf.float32)
logits = tf.layers.dense(inputs=final_states.h, units=1)
</code></pre>
<p></p><h3 id="pre-trained-vectors">Pre-trained vectors</h3><p>Most of the models that we have shown before rely on word embeddings as a first layer. So far, we have initialized this embedding layer randomly. However, <a href="https://arxiv.org/abs/1607.01759" rel="nofollow noopener">much</a> <a href="https://arxiv.org/abs/1301.3781" rel="nofollow noopener">previous</a> <a href="https://arxiv.org/abs/1103.0398" rel="nofollow noopener">work</a> has shown that using embeddings pre-trained on a large unlabeled corpus as initialization is beneficial, particularly when training on only a small number of labeled examples. The most popular pre-trained embedding is <a href="https://www.tensorflow.org/tutorials/word2vec" rel="nofollow noopener">word2vec</a>. Leveraging knowledge from unlabeled data via pre-trained embeddings is an instance of <a href="http://ruder.io/transfer-learning/" rel="nofollow noopener"><em>transfer learning</em></a>.</p><p>To this end, we will show you how to use them in an <code>Estimator</code>. We will use the pre-trained vectors from another popular model, <a href="https://nlp.stanford.edu/projects/glove/" rel="nofollow noopener">GloVe</a>.</p><pre><code class="language-python">embeddings = {}
with open('glove.6B.50d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.strip().split()
        w = values[0]
        vectors = np.asarray(values[1:], dtype='float32')
        embeddings[w] = vectors
</code></pre>
<p>After loading the vectors into memory from a file we store them as a <code>numpy.array</code> using the same indexes as our vocabulary. The created array is of shape <code>(5000, 50)</code>. At every row index, it contains the 50-dimensional vector representing the word at the same index in our vocabulary.</p><pre><code class="language-python">embedding_matrix = np.random.uniform(-1, 1, size=(vocab_size, embedding_size))
for w, i in word_index.items():
    v = embeddings.get(w)
    if v is not None and i &lt; vocab_size:
        embedding_matrix[i] = v
</code></pre>
<p>Finally, we can use a custom initializer function and pass it in the <code>params</code>object to our <code>cnn_model_fn</code> , without any modifications.</p><pre><code class="language-python">def my_initializer(shape=None, dtype=tf.float32, partition_info=None):
    assert dtype is tf.float32
    return embedding_matrix
params = {'embedding_initializer': my_initializer}
cnn_pretrained_classifier = tf.estimator.Estimator(
    model_fn=cnn_model_fn,
    model_dir=os.path.join(model_dir, 'cnn_pretrained'),
    params=params)
train_and_evaluate(cnn_pretrained_classifier)
</code></pre>
<h3 id="running-tensorboard">Running TensorBoard</h3><p>Now we can launch TensorBoard and see how the different models we’ve trained compare against each other in terms of training time and performance.</p><p>In a terminal, we run</p><p><code>&gt; tensorboard --logdir={model_dir}</code></p>
<p>We can visualize many metrics collected while training and testing, including the loss function values of each model at each training step, and the precision-recall curves. This is of course most useful to select which model works best for our use-case as well as how to choose classification thresholds.</p><figure class="kg-card kg-image-card"><amp-img src="https://cdn-images-1.medium.com/max/1000/1*ACyqHB1zNYZQ3YDTiY7WPg.png" class="kg-image" alt width="1000" height="393" layout="responsive"></amp-img></figure><figure class="kg-card kg-image-card kg-card-hascaption"><amp-img src="https://cdn-images-1.medium.com/max/750/1*_tL-oN9IvzDCWyVGZ4LF3A.png" class="kg-image" alt width="750" height="431" layout="responsive"></amp-img><figcaption>Training loss across steps on the left and Precision-Recall curves on the test data for each of our models on the left</figcaption></figure><h3 id="getting-predictions">Getting Predictions</h3><p>To obtain predictions on new sentences we can use the <code>predict</code> method in the <code>Estimator</code> instances, which will load the latest checkpoint for each model and evaluate on the unseen examples. But before passing the data into the model we have to clean up, tokenize and map each token to the corresponding index as we see below.</p><pre><code class="language-python">def text_to_index(sentence):
    # Remove punctuation characters except for the apostrophe
    translator = str.maketrans('', '', string.punctuation.replace("'", ''))
    tokens = sentence.translate(translator).lower().split()
    return np.array([1] + [word_index[t] if t in word_index else 2 for t in tokens])

def print_predictions(sentences, classifier):
    indexes = [text_to_index(sentence) for sentence in sentences]
    x = sequence.pad_sequences(indexes, 
                               maxlen=sentence_size, 
                               padding='post', 
                               value=0)
    length = np.array([min(len(x), sentence_size) for x in indexes])
    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={"x": x, "len": length}, shuffle=False)
    predictions = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]
    print(predictions)
</code></pre>
<p>It is worth noting that the checkpoint itself is not sufficient to make predictions; the actual code used to build the estimator is necessary as well in order to map the saved weights to the corresponding tensors. It’s a good practice to associate saved checkpoints with the branch of code with which they were created.</p><p>If you are interested in exporting the models to disk in a fully recoverable way, you might want to look into the <a href="https://www.tensorflow.org/programmers_guide/saved_model#using_savedmodel_with_estimators" rel="nofollow noopener">SavedModel</a> class, which is especially useful for serving your model through an API using <a href="https://github.com/tensorflow/serving" rel="nofollow noopener">TensorFlow Serving</a> or loading it in the browser with <a href="https://js.tensorflow.org/" rel="nofollow noopener">TensorFlow.js</a>.</p><p>In this blog post, we explored how to use estimators for text classification, in particular for the IMDB Reviews Dataset. We trained and visualized our own embeddings, as well as loaded pre-trained ones. We started from a simple baseline and made our way to convolutional neural networks and LSTMs.</p><p>For more details, be sure to check out:</p><ul><li>A <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.ipynb" rel="nofollow noopener">Jupyter Notebook</a> that can run locally, or on <a href="https://goo.gl/fXsCra" rel="nofollow noopener">Colaboratory</a>.</li><li>The complete <a href="https://github.com/eisenjulian/nlp_estimator_tutorial/blob/master/nlp_estimators.py" rel="nofollow noopener">source code</a> for this blog post.</li><li>The TensorFlow <a href="https://www.tensorflow.org/programmers_guide/embedding" rel="nofollow noopener">Embedding</a> guide.</li><li>The TensorFlow <a href="https://www.tensorflow.org/tutorials/word2vec" rel="nofollow noopener">Vector Representation of Words</a> tutorial.</li><li>The <em>NLTK</em> <a href="http://www.nltk.org/book/ch03.html" rel="nofollow noopener">Processing Raw Text</a> chapter on how to design langage pipelines.</li></ul><hr></hr><p>Thanks for reading! If you like you can find us online at <a href="http://ruder.io/" rel="nofollow noopener nofollow noopener nofollow noopener">ruder.io</a> and <a href="https://twitter.com/eisenjulian" rel="nofollow noopener nofollow noopener">@eisenjulian</a>. Send our way all your feedback and questions.</p>

            </section>

        </article>
    </main>
    <footer class="page-footer">
        <h3>Julian Eisenschlos</h3>
            <p>Machine Learning, Natural Language Processing, Math • AI Research @ Google • Co-founder botmaker.com • Previously Facebook &amp; ASAPP</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a></span>
    </footer>
    
</body>
</html>