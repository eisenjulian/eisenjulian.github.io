<!DOCTYPE html>
<html lang="en">
<head>

    <title>Learning to Reason Over Tables from Less Data</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="preload" as="style" href="../assets/built/screen.css%3Fv=f60aed1d93.css">
    <link rel="preload" as="script" href="../assets/built/source.js%3Fv=f60aed1d93">

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css%3Fv=f60aed1d93.css">

    <style>
        :root {
            --background-color: #ffffff
        }
    </style>

    <script>
        /* The script for calculating the color contrast has been taken from
        https://gomakethings.com/dynamically-changing-the-text-color-based-on-background-color-contrast-with-vanilla-js/ */
        var accentColor = getComputedStyle(document.documentElement).getPropertyValue('--background-color');
        accentColor = accentColor.trim().slice(1);
        var r = parseInt(accentColor.substr(0, 2), 16);
        var g = parseInt(accentColor.substr(2, 2), 16);
        var b = parseInt(accentColor.substr(4, 2), 16);
        var yiq = ((r * 299) + (g * 587) + (b * 114)) / 1000;
        var textColor = (yiq >= 128) ? 'dark' : 'light';

        document.documentElement.className = `has-${textColor}-text`;
    </script>

    <meta name="description" content="In &quot;Understanding tables with intermediate pre-training&quot;, published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data.">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Julian Eisenschlos">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Learning to Reason Over Tables from Less Data">
    <meta property="og:description" content="In &quot;Understanding tables with intermediate pre-training&quot;, published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data.">
    <meta property="og:url" content="https://eisenjulian.github.io/learning-to-reason-over-tables-from-less-data/">
    <meta property="og:image" content="https://eisenjulian.github.io/content/images/2021/04/few-shot.png">
    <meta property="article:published_time" content="2021-04-10T11:44:58.000Z">
    <meta property="article:modified_time" content="2024-03-17T19:26:15.000Z">
    <meta property="article:tag" content="Natural Language Processing">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Learning to Reason Over Tables from Less Data">
    <meta name="twitter:description" content="In &quot;Understanding tables with intermediate pre-training&quot;, published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data.">
    <meta name="twitter:url" content="https://eisenjulian.github.io/learning-to-reason-over-tables-from-less-data/">
    <meta name="twitter:image" content="https://eisenjulian.github.io/content/images/2021/04/few-shot.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Julian Eisenschlos">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Natural Language Processing">
    <meta name="twitter:site" content="@eisenjulian">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Julian Eisenschlos",
        "url": "https://eisenjulian.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Julian Eisenschlos",
        "image": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/content/images/2024/03/perfil-square.jpeg",
            "width": 710,
            "height": 710
        },
        "url": "https://eisenjulian.github.io/author/julian/",
        "sameAs": []
    },
    "headline": "Learning to Reason Over Tables from Less Data",
    "url": "https://eisenjulian.github.io/learning-to-reason-over-tables-from-less-data/",
    "datePublished": "2021-04-10T11:44:58.000Z",
    "dateModified": "2024-03-17T19:26:15.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://eisenjulian.github.io/content/images/2021/04/few-shot.png"
    },
    "keywords": "Natural Language Processing",
    "description": "In &quot;Understanding tables with intermediate pre-training&quot;, published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data.",
    "mainEntityOfPage": "https://eisenjulian.github.io/learning-to-reason-over-tables-from-less-data/"
}
    </script>

    <meta name="generator" content="Ghost 5.80">
    <link rel="alternate" type="application/rss+xml" title="Julian Eisenschlos" href="../rss/index.html">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="3948e65ad8bc7936f3efbbea1e" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://eisenjulian.github.io/" crossorigin="anonymous"></script>
    
    <link href="https://eisenjulian.github.io/webmentions/receive/" rel="webmention">
    <script defer src="../public/cards.min.js%3Fv=f60aed1d93"></script>
    <link rel="stylesheet" type="text/css" href="../public/cards.min.css%3Fv=f60aed1d93.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-136394483-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-136394483-1');
</script>

<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
<style>
  .gh-footer-bar {display: none; !important}
</style><style>:root {--ghost-accent-color: #15171A;}</style>

</head>
<body class="post-template tag-natural-language-processing tag-hash-import-2024-03-17-18-37 has-sans-title has-sans-body">

<div class="gh-viewport">
    
    <header id="gh-navigation" class="gh-navigation is-left-logo has-accent-color gh-outer">
    <div class="gh-navigation-inner gh-inner">

        <div class="gh-navigation-brand">
            <a class="gh-navigation-logo is-title" href="../index.html">
                    Julian Eisenschlos
            </a>
            <button class="gh-search gh-icon-button" aria-label="Search this site" data-ghost-search>
    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>            <button class="gh-burger gh-icon-button" aria-label="Menu">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 256 256"><path d="M224,128a8,8,0,0,1-8,8H40a8,8,0,0,1,0-16H216A8,8,0,0,1,224,128ZM40,72H216a8,8,0,0,0,0-16H40a8,8,0,0,0,0,16ZM216,184H40a8,8,0,0,0,0,16H216a8,8,0,0,0,0-16Z"></path></svg>                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 256 256"><path d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"></path></svg>            </button>
        </div>

        <nav class="gh-navigation-menu">
            <ul class="nav">
    <li class="nav-about"><a href="../about/index.html">About</a></li>
    <li class="nav-papers"><a href="../publications/index.html">Papers</a></li>
    <li class="nav-talks"><a href="../talks/index.html">Talks</a></li>
    <li class="nav-github"><a href="https://github.com/eisenjulian">GitHub</a></li>
    <li class="nav-linkedin"><a href="https://www.linkedin.com/in/eisenjulian">LinkedIn</a></li>
    <li class="nav-x"><a href="https://x.com/eisenjulian">X</a></li>
</ul>

        </nav>

        <div class="gh-navigation-actions">
                    <button class="gh-search gh-icon-button" aria-label="Search this site" data-ghost-search>
    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>        </div>

    </div>
</header>

    

<main class="gh-main">

    <article class="gh-article post tag-natural-language-processing tag-hash-import-2024-03-17-18-37">

        <header class="gh-article-header gh-canvas">

                <a class="gh-article-tag" href="../tag/natural-language-processing/index.html">Natural Language Processing</a>
            <h1 class="gh-article-title is-title">Learning to Reason Over Tables from Less Data</h1>
                <p class="gh-article-excerpt is-body">In &quot;Understanding tables with intermediate pre-training&quot;, published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data.</p>

            <div class="gh-article-meta">
                <div class="gh-article-author-image">
                            <a href="../author/julian/index.html">
                                <img class="author-profile-image" src="../content/images/size/w160/2024/03/perfil-square.jpeg" alt="Julian Eisenschlos" />
                            </a>
                </div>
                <div class="gh-article-meta-wrapper">
                    <h4 class="gh-article-author-name"><a href="../author/julian/index.html">Julian Eisenschlos</a></h4>
                    <div class="gh-article-meta-content">
                        <time class="gh-article-meta-date" datetime="2021-04-10">Apr 10, 2021</time>
                            <span class="gh-article-meta-length"><span class="bull">—</span> 7 min read</span>
                    </div>
                </div>
            </div>

                <figure class="gh-article-image">
        <img
            srcset="../content/images/size/w320/2021/04/few-shot.png 320w,
                   ../content/images/size/w600/2021/04/few-shot.png 600w,
                  ../content/images/size/w960/2021/04/few-shot.png 960w,
                 ../content/images/size/w1200/2021/04/few-shot.png 1200w,
                ../content/images/size/w2000/2021/04/few-shot.png 2000w"
            src="../content/images/size/w1200/2021/04/few-shot.png"
            alt="Learning to Reason Over Tables from Less Data"
        >
    </figure>

        </header>

        <section class="gh-content gh-canvas is-body">
            <p><em>The task of recognizing </em><a href="https://en.wikipedia.org/wiki/Textual_entailment?ref=localhost"><em>textual entailment</em></a><em>, also known as natural language inference, consists of determining whether a piece of text (a premise), can be implied or contradicted (or neither) by another piece of text (the hypothesis). While this problem is often considered an important test for the reasoning skills of machine learning (ML) systems and has been studied in depth for plain text inputs, much less effort has been put into applying such models to structured data, such as websites, tables, databases, etc. Yet, recognizing textual entailment is especially relevant whenever the contents of a table need to be accurately summarized and presented to a user, and is essential for high fidelity </em><a href="https://en.wikipedia.org/wiki/Question_answering?ref=localhost"><em>question answering</em></a><em> systems and </em><a href="https://en.wikipedia.org/wiki/Virtual_assistant?ref=localhost"><em>virtual assistants</em></a><em>.</em></p><p><em>In "</em><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.27/?ref=localhost"><em>Understanding tables with intermediate pre-training</em></a><em>", published in </em><a href="https://2020.emnlp.org/papers/findings?ref=localhost"><em>Findings of EMNLP 2020</em></a><em>, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data. We build upon our earlier </em><a href="https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html?ref=localhost"><em>TAPAS</em></a><em> model, which was an extension of the </em><a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html?ref=localhost"><em>BERT</em></a><em> bi-directional </em><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html?ref=localhost"><em>Transformer</em></a><em> model with special embeddings to find answers in tables. Applying our new pre-training objectives to TAPAS yields a new state of the art on multiple datasets involving tables. On </em><a href="https://tabfact.github.io/?ref=localhost"><em>TabFact</em></a><em>, for example, it reduces the gap between model and human performance by ~50%. We also systematically benchmark methods of selecting relevant input for higher efficiency, achieving 4x gains in speed and memory, while retaining 92% of the results. All the models for different tasks and sizes are released on </em><a href="https://github.com/google-research/tapas?ref=localhost"><em>GitHub repo</em></a><em>, where you can try them out yourself in a </em><a href="http://tiny.cc/tapas-tabfact-colab?ref=localhost"><em>colab</em></a><em> Notebook.</em></p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://ai.googleblog.com/2021/01/learning-to-reason-over-tables-from.html?ref=localhost"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Learning to Reason Over Tables from Less Data</div><div class="kg-bookmark-description">Posted by Julian Eisenschlos, AI Resident, Google Research, Zürich The task of recognizing textual entailment , also known as natural la...</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://ai.googleblog.com/favicon.ico" alt=""><span class="kg-bookmark-author">Google AI Blog</span></div></div><div class="kg-bookmark-thumbnail"><img src="http://2.bp.blogspot.com/-qRz-hnwUdY4/WulXSQ6Rv4I/AAAAAAAATvQ/shk7KsphA0c3E3nUMsDVASqYaH0PhLPNwCK4BGAYYCw/s1600/GoogleAI_logo_horizontal_color_rgb.png" alt=""></div></a><figcaption><p><span style="white-space: pre-wrap;">Cross posted from the Google AI blog</span></p></figcaption></figure><hr><h3 id="textual-entailment">Textual Entailment</h3><p>The task of textual entailment is more challenging when applied to tabular data than plain text. Consider, for example, a table from Wikipedia with some sentences derived from its associated table content. Assessing if the content of the table entails or contradicts the sentence may require looking over multiple columns and rows, and possibly performing simple numeric computations, like averaging, summing, differencing, etc.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-TOSTbdc9ndk/YBRIuPjyCZI/AAAAAAAAHEw/NYIj9usvgaQu1Fm9UfBJPL_12M6Y0F5fgCLcBGAsYHQ/w400-h184/image5.png" class="kg-image" alt="" loading="lazy" width="400" height="184"><figcaption><span style="white-space: pre-wrap;">A table together with some statements from </span><a href="https://tabfact.github.io/?ref=localhost"><span style="white-space: pre-wrap;">TabFact</span></a><span style="white-space: pre-wrap;">. The content of the table can be used to support or contradict the statements.</span></figcaption></figure><p>Following the methods used by TAPAS, we encode the content of a statement and a table together, pass them through a Transformer model, and obtain a single number with the probability that the statement is entailed or refuted by the table.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-zaz8_g9XvxI/YBRJAP21w4I/AAAAAAAAHE4/CvLDXDybaiQs9QNwEdcwW6KaLvyUeJUewCLcBGAsYHQ/w400-h255/TAPAS.jpg" class="kg-image" alt="" loading="lazy" width="400" height="255"><figcaption><span style="white-space: pre-wrap;">The </span><a href="https://www.aclweb.org/anthology/2020.acl-main.398?ref=localhost"><span style="white-space: pre-wrap;">TAPAS model</span></a><span style="white-space: pre-wrap;"> architecture uses a BERT model to encode the statement and the flattened table, read row by row. Special embeddings are used to encode the table structure. The vector output of the first token is used to predict the probability of entailment.</span></figcaption></figure><p>Because the only information in the training examples is a binary value (i.e., "correct" or "incorrect"), training a model to understand whether a statement is entailed or not is challenging and highlights the difficulty in achieving generalization in deep learning, especially when the provided training signal is scarce. Seeing isolated entailed or refuted examples, a model can easily pick-up on spurious patterns in the data to make a prediction, for example the presence of the word "tie" in "Greg Norman and Billy Mayfair tie in rank", instead of truly comparing their ranks, which is what is needed to successfully apply the model beyond the original training data.</p><h3 id="pre-training-tasks">Pre-training Tasks</h3><p>Pre-training tasks can be used to “warm-up” models by providing them with large amounts of readily available unlabeled data. However, pre-training typically includes primarily plain text and not tabular data. In fact, TAPAS was originally pre-trained using a simple masked language modelling objective that was not designed for tabular data applications. In order to improve the model performance on tabular data, we introduce two novel pretraining binary-classification tasks called <em>counterfactual</em> and <em>synthetic</em>, which can be applied as a second stage of pre-training (often called <em>intermediate pre-training</em>).</p><p>In the counterfactual task, we source sentences from Wikipedia that mention an entity (person, place or thing) that also appears in a given table. Then, 50% of the time, we modify the statement by swapping the entity for another alternative. To make sure the statement is realistic, we choose a replacement among the entities in the same column in the table. The model is trained to recognize whether the statement was modified or not. This pre-training task includes millions of such examples, and although the reasoning about them is not complex, they typically will still sound natural.</p><p>For the synthetic task, we follow a method similar to <a href="https://www.aclweb.org/anthology/P15-1129/?ref=localhost">semantic parsing</a> in which we generate statements using a simple set of grammar rules that require the model to understand basic mathematical operations, such as sums and averages (e.g., "the sum of earnings"), or to understand how to filter the elements in the table using some condition (e.g.,"the country is Australia"). Although these statements are artificial, they help improve the numerical and logical reasoning skills of the model.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-kLdqZMbPSEY/YBRJIn_88tI/AAAAAAAAHE8/MBhRtGDgqKIATplbDWbad6sRniHgjmLhgCLcBGAsYHQ/w400-h75/image1.png" class="kg-image" alt="" loading="lazy" width="400" height="75"><figcaption><span style="white-space: pre-wrap;">Example instances for the two novel pre-training tasks. </span><b><strong style="white-space: pre-wrap;">Counterfactual</strong></b><span style="white-space: pre-wrap;"> examples swap entities mentioned in a sentence that accompanies the input table for a plausible alternative. </span><b><strong style="white-space: pre-wrap;">Synthetic</strong></b><span style="white-space: pre-wrap;"> statements use grammar rules to create new sentences that require combining the information of the table in complex ways.</span></figcaption></figure><h3 id="results">Results</h3><p>We evaluate the success of the counterfactual and synthetic pre-training objectives on the TabFact dataset by comparing to the baseline TAPAS model and to two prior models that have exhibited success in the textual entailment domain, <a href="https://www.aclweb.org/anthology/2020.acl-main.539/?ref=localhost">LogicalFactChecker</a> (LFC) and <a href="https://www.aclweb.org/anthology/2020.emnlp-main.126/?ref=localhost">Structure Aware Transformer</a> (SAT). The baseline TAPAS model exhibits improved performance relative to LFC and SAT, but the pre-trained model (TAPAS+CS) performs significantly better, achieving a new state of the art.</p><p>We also apply TAPAS+CS to question answering tasks on the <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54253&ref=localhost">SQA dataset</a>, which requires that the model find answers from the content of tables in a dialog setting. The inclusion of CS objectives improves the previous best performance by more than 4 points, demonstrating that this approach also generalizes performance beyond just textual entailment.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-gRLgwMzDwyQ/YBRJcOPxUFI/AAAAAAAAHFI/cVl0tAGeEX4TLCLt-NvUS7Nfq_FBHy81QCLcBGAsYHQ/w400-h229/Accuracy.png" class="kg-image" alt="" loading="lazy" width="400" height="228"><figcaption><span style="white-space: pre-wrap;">Results on TabFact (</span><b><strong style="white-space: pre-wrap;">left</strong></b><span style="white-space: pre-wrap;">) and SQA (</span><b><strong style="white-space: pre-wrap;">right</strong></b><span style="white-space: pre-wrap;">). Using the synthetic and counterfactual datasets, we achieve new state-of-the-art results in both tasks by a large margin.</span></figcaption></figure><h3 id="data-and-compute-efficiency">Data and Compute Efficiency</h3><p>Another aspect of the counterfactual and synthetic pre-training tasks is that since the models are already tuned for binary classification, they can be applied without any fine-tuning to TabFact. We explore what happens to each of the models when trained only on a subset (or even none) of the data. Without looking at a single example, the TAPAS+CS model is competitive with a strong baseline Table-Bert, and when only 10% of the data are included, the results are comparable to the previous state-of-the-art.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-ij2gSWPrt8U/YBRJkWHIfVI/AAAAAAAAHFM/NZWkx9y0f94Gg_FjBLH0-MOXFHYiJANGgCLcBGAsYHQ/w400-h236/image7.png" class="kg-image" alt="" loading="lazy" width="399" height="236"><figcaption><span style="white-space: pre-wrap;">Dev accuracy on TabFact relative to the fraction of the training data used.</span></figcaption></figure><p>A general concern when trying to use large models such as this to operate on tables, is that their high computational requirements makes it difficult for them to parse very large tables. To address this, we investigate whether one can heuristically select subsets of the input to pass through the model in order to optimize its computational efficiency.</p><p>We conducted a systematic study of different approaches to filter the input and discovered that simple methods that select for word overlap between a full column and the subject statement give the best results. By dynamically selecting which tokens of the input to include, we can use fewer resources or work on larger inputs at the same cost. The challenge is doing so without losing important information and hurting accuracy.</p><p>For instance, the models discussed above all use sequences of 512 tokens, which is around the normal limit for a transformer model (although recent efficiency methods like the <a href="https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html?ref=localhost">Reformer</a> or <a href="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html?ref=localhost">Performer</a> are proving effective in scaling the input size). The column selection methods we propose here can allow for faster training while still achieving high accuracy on TabFact. For 256 input tokens we get a very small drop in accuracy, but the model can now be pre-trained, fine-tuned and make predictions up to two times faster. With 128 tokens the model still outperforms the previous state-of-the-art model, with an even more significant speed-up — 4x faster across the board.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://1.bp.blogspot.com/-LjZgvsrnh7E/YBRJpZUYYUI/AAAAAAAAHFQ/-b-_V4JCJq0S4S_k6vNcqVlRSvfPx6WwwCLcBGAsYHQ/w400-h236/image6.png" class="kg-image" alt="" loading="lazy" width="399" height="236"><figcaption><span style="white-space: pre-wrap;">Accuracy on TabFact using different sequence lengths, by shortening the input with our column selection method.</span></figcaption></figure><p>Using both the column selection method we proposed and the novel pre-training tasks, we can create table parsing models that need fewer data and less compute power to obtain better results.</p><p>We have made available the new models and pre-training techniques at <a href="https://github.com/google-research/tapas?ref=localhost">our GitHub repo</a>, where you can try it out yourself in <a href="http://tiny.cc/tapas-tabfact-colab?ref=localhost">colab</a>. In order to make this approach more accessible, we also shared models of varying sizes all the way down to “<a href="https://arxiv.org/abs/1908.08962?ref=localhost">tiny</a>”. It is our hope that these results will help spur development of table reasoning among the broader research community.</p><h3 id="acknowledgements">Acknowledgements</h3><p><em>This work was carried out by Julian Martin Eisenschlos, Syrine Krichene and Thomas Müller from our </em><a href="https://research.google/teams/language/?ref=localhost"><em>Language Team</em></a><em> in Zürich. We would like to thank Jordan Boyd-Graber, Yasemin Altun, Emily Pitler, Benjamin Boerschinger, Srini Narayanan, Slav Petrov, William Cohen and Jonathan Herzig for their useful comments and suggestions.</em></p>
        </section>

    </article>


</main>


            <section class="gh-container is-grid gh-outer">
                <div class="gh-container-inner gh-inner">
                    <h2 class="gh-container-title">Read more</h2>
                    <div class="gh-feed">
                            <article class="gh-card post no-image">
    <a class="gh-card-link" href="../foundation-models-for-reasoning-on-charts/index.html">
            <figure class="gh-card-image">
                <img
                    srcset="../content/images/size/w160/format/webp/2024/03/deplot_front_page_self_referential_figure_v2.jpg 160w,
                           ../content/images/size/w320/format/webp/2024/03/deplot_front_page_self_referential_figure_v2.jpg 320w,
                          ../content/images/size/w600/format/webp/2024/03/deplot_front_page_self_referential_figure_v2.jpg 600w,
                         ../content/images/size/w960/format/webp/2024/03/deplot_front_page_self_referential_figure_v2.jpg 960w,
                        ../content/images/size/w1200/format/webp/2024/03/deplot_front_page_self_referential_figure_v2.jpg 1200w,
                       ../content/images/size/w2000/format/webp/2024/03/deplot_front_page_self_referential_figure_v2.jpg2.jpg 2000w"
                    sizes="320px"
                    src="../content/images/size/w600/2024/03/deplot_front_page_self_referential_figure_v2.jpg"
                    alt="Foundation models for reasoning on charts"
                    loading="lazy"
                >
            </figure>
        <div class="gh-card-wrapper">
            <h3 class="gh-card-title is-title">Foundation models for reasoning on charts</h3>
                    <p class="gh-card-excerpt is-body">Visual language is the form of communication that relies on pictorial symbols outside of text to convey information. It is ubiquitous in our digital life in the form of iconography, infographics, tables, plots, and charts, extending to the real world in street signs, comic books, food labels, etc. For that</p>
            <footer class="gh-card-meta">
<!--
             -->                    <time class="gh-card-date" datetime="2024-03-17">Mar 17, 2024</time>
                <!--
         --></footer>
        </div>
    </a>
</article>                            <article class="gh-card post no-image">
    <a class="gh-card-link" href="../multifit/index.html">
            <figure class="gh-card-image">
                <img
                    srcset="../content/images/size/w160/format/webp/2020/08/multifit_bootstrapping.png 160w,
                           ../content/images/size/w320/format/webp/2020/08/multifit_bootstrapping.png 320w,
                          ../content/images/size/w600/format/webp/2020/08/multifit_bootstrapping.png 600w,
                         ../content/images/size/w960/format/webp/2020/08/multifit_bootstrapping.png 960w,
                        ../content/images/size/w1200/format/webp/2020/08/multifit_bootstrapping.png 1200w,
                       ../content/images/size/w2000/format/webp/2020/08/multifit_bootstrapping.png.png 2000w"
                    sizes="320px"
                    src="../content/images/size/w600/2020/08/multifit_bootstrapping.png"
                    alt="Efficient multi-lingual language model fine-tuning"
                    loading="lazy"
                >
            </figure>
        <div class="gh-card-wrapper">
            <h3 class="gh-card-title is-title">Efficient multi-lingual language model fine-tuning</h3>
                <p class="gh-card-excerpt is-body">Our latest paper studies multilingual text classification and introduces MultiFiT, a novel method based on ULMFiT. MultiFiT, trained on 100 labeled documents in the target language, outperforms multi-lingual BERT, and the LASER algorithm—even though LASER requires a corpus of parallel texts.</p>
            <footer class="gh-card-meta">
<!--
             -->                    <time class="gh-card-date" datetime="2019-09-10">Sep 10, 2019</time>
                <!--
         --></footer>
        </div>
    </a>
</article>                            <article class="gh-card post no-image">
    <a class="gh-card-link" href="../deep-learning-in-100-lines/index.html">
            <figure class="gh-card-image">
                <img
                    srcset="../content/images/size/w160/format/webp/2019/03/onion.jpg 160w,
                           ../content/images/size/w320/format/webp/2019/03/onion.jpg 320w,
                          ../content/images/size/w600/format/webp/2019/03/onion.jpg 600w,
                         ../content/images/size/w960/format/webp/2019/03/onion.jpg 960w,
                        ../content/images/size/w1200/format/webp/2019/03/onion.jpg 1200w,
                       ../content/images/size/w2000/format/webp/2019/03/onion.jpgn.jpg 2000w"
                    sizes="320px"
                    src="../content/images/size/w600/2019/03/onion.jpg"
                    alt="Neural Networks in 100 lines of pure Python"
                    loading="lazy"
                >
            </figure>
        <div class="gh-card-wrapper">
            <h3 class="gh-card-title is-title">Neural Networks in 100 lines of pure Python</h3>
                <p class="gh-card-excerpt is-body">Can we build a Deep learning framework in plain Python and Numpy? Can we make it compact, clear and extendable? Let&#x27;s set out to explore those ideas and see what we can create!</p>
            <footer class="gh-card-meta">
<!--
             -->                    <time class="gh-card-date" datetime="2019-03-17">Mar 17, 2019</time>
                <!--
         --></footer>
        </div>
    </a>
</article>                            <article class="gh-card post no-image">
    <a class="gh-card-link" href="../text-classification-with-estimators/index.html">
            <figure class="gh-card-image">
                <img
                    srcset="../content/images/size/w160/format/webp/2019/03/estimators_loss.png 160w,
                           ../content/images/size/w320/format/webp/2019/03/estimators_loss.png 320w,
                          ../content/images/size/w600/format/webp/2019/03/estimators_loss.png 600w,
                         ../content/images/size/w960/format/webp/2019/03/estimators_loss.png 960w,
                        ../content/images/size/w1200/format/webp/2019/03/estimators_loss.png 1200w,
                       ../content/images/size/w2000/format/webp/2019/03/estimators_loss.pngs.png 2000w"
                    sizes="320px"
                    src="../content/images/size/w600/2019/03/estimators_loss.png"
                    alt="Text Classification with TensorFlow Estimators"
                    loading="lazy"
                >
            </figure>
        <div class="gh-card-wrapper">
            <h3 class="gh-card-title is-title">Text Classification with TensorFlow Estimators</h3>
                <p class="gh-card-excerpt is-body">Throughout this post we will explain how to classify text using Estimators,  Datasets and Feature Columns, with a scalable high-level API in TensorFlow. </p>
            <footer class="gh-card-meta">
<!--
             -->                    <time class="gh-card-date" datetime="2018-03-07">Mar 7, 2018</time>
                <!--
         --></footer>
        </div>
    </a>
</article>                    </div>
                </div>
            </section>

    
    <footer class="gh-footer has-accent-color gh-outer">
    <div class="gh-footer-inner gh-inner">

        <div class="gh-footer-bar">
            <span class="gh-footer-logo is-title">
                    Julian Eisenschlos
            </span>
            <nav class="gh-footer-menu">
                
            </nav>
            <div class="gh-footer-copyright">
                Powered by <a href="https://ghost.org/" target="_blank" rel="noopener">Ghost</a>
            </div>
        </div>


    </div>
</footer>    
</div>

    <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="pswp__bg"></div>

    <div class="pswp__scroll-wrap">
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>
<script src="../assets/built/source.js%3Fv=f60aed1d93"></script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/prism.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-lua.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-c.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-cpp.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-latex.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-python.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/components/prism-markdown.min.js"></script>

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    }
});
</script>

</body>
</html>
