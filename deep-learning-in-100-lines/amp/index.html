<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">

    <title>Deep Learning in 100 lines of plain Python</title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Julian Eisenschlos" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Deep Learning in 100 lines of plain Python" />
    <meta property="og:description" content="Can we build a Deep learning framework in plain Python and Numpy? Can we make it compact, clear and extendable? Let&#x27;s set out to explore those ideas and see what we can create!" />
    <meta property="og:url" content="https://eisenjulian.github.io/deep-learning-in-100-lines/" />
    <meta property="og:image" content="https://eisenjulian.github.io/content/images/2019/03/onion.jpg" />
    <meta property="article:published_time" content="2019-03-17T15:33:37.000Z" />
    <meta property="article:modified_time" content="2019-03-21T14:56:43.000Z" />
    <meta property="article:tag" content="Automatic Differentiation" />
    <meta property="article:tag" content="Math" />
    <meta property="article:tag" content="Tutorials" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Deep Learning in 100 lines of plain Python" />
    <meta name="twitter:description" content="Can we build a Deep learning framework in plain Python and Numpy? Can we make it compact, clear and extendable? Let&#x27;s set out to explore those ideas and see what we can create!" />
    <meta name="twitter:url" content="https://eisenjulian.github.io/deep-learning-in-100-lines/" />
    <meta name="twitter:image" content="https://eisenjulian.github.io/content/images/2019/03/onion.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Julian Eisenschlos" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Automatic Differentiation, Math, Tutorials" />
    <meta name="twitter:site" content="@eisenjulian" />
    <meta property="og:image:width" content="640" />
    <meta property="og:image:height" content="360" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Julian Eisenschlos",
        "logo": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Julian Eisenschlos",
        "image": {
            "@type": "ImageObject",
            "url": "https://eisenjulian.github.io/content/images/2019/03/perfil-square.jpeg",
            "width": 710,
            "height": 710
        },
        "url": "https://eisenjulian.github.io/author/julian/",
        "sameAs": []
    },
    "headline": "Deep Learning in 100 lines of plain Python",
    "url": "https://eisenjulian.github.io/deep-learning-in-100-lines/",
    "datePublished": "2019-03-17T15:33:37.000Z",
    "dateModified": "2019-03-21T14:56:43.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://eisenjulian.github.io/content/images/2019/03/onion.jpg",
        "width": 640,
        "height": 360
    },
    "keywords": "Automatic Differentiation, Math, Tutorials",
    "description": "Can we build a Deep learning framework in plain Python and Numpy? Can we make it compact, clear and extendable? Let&#x27;s set out to explore those ideas and see what we can create!",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://eisenjulian.github.io/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.18" />
    <link rel="alternate" type="application/rss+xml" title="Julian Eisenschlos" href="../../rss/index.html" />

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,600,400" />
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-anim" src="https://cdn.ampproject.org/v0/amp-anim-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../index.html">Julian Eisenschlos</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Deep Learning in 100 lines of plain Python</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/julian/index.html">Julian Eisenschlos</a></p>
                    <time class="post-date" datetime="2019-03-17">2019-03-17</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://eisenjulian.github.io/content/images/2019/03/onion.jpg" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p><em>Can we build a Deep learning framework in plain Python and Numpy? Can we make it compact, clear and extendable? Let's set out to explore those ideas and see what we can create!</em></p><hr></hr><p>In today's day and age there are multiple frameworks to choose from, with various important features such as auto-differentiation, graph-based optimized computation and hardware acceleration.</p><p>What we'll cover:</p><ul><li>Automatic reverse-mode differentiation</li><li>How to implement a few basic layers</li><li>How to create a training loop</li></ul><p>The design choices are heavily based on <a href="https://github.com/explosion/thinc">Thinc</a>'s Deep Learning library, since they came up with the smart idea of using anonymous functions, closures, and the stack to share and keep track of intermediate results while calculating gradients. This post is also inspired by Jeremy Howard's great <a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">PyTorch tutorial</a>, but taking it one more step towards the bottom getting rid of <code>autograd</code>.</p><h2 id="bring-on-the-math">Bring on the math</h2><p>You probably heard this already, but the back-propagation algorithm, originally described in the 60's and 70's is the application of the chain rule to calculate gradients of a real function with respect of its various parameters.</p><p>The end goal is to find a minimum (hopefully global) of a function by taking steps in the opposite direction of said gradient. This is how it looks when we have two parameters to optimize.</p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>Source: <a href="https://jed-ai.github.io/py1_gd_animation/">7 Simple Steps To Visualize And Animate The Gradient Descent Algorithm</a></figcaption></figure><p>That is, if we have two vector spaces $V$ and $W$ and two differentiable functions $f:V\to W$ and $g:W\to\mathbb{R}$ we have that if $v\in V$ and $f(v) = w$</p><p>$\nabla g \circ f(v) = \nabla g(w) \cdot Jf(v)$</p><p>where the gradient $\nabla$ of a function into $\mathbb{R}$ is the row vector function consisting of all the partial derivatives and the Jacobian $J$ is a generalization of the gradient for functions with multi-dimensional output, represents a matrix function where each row is the gradient of the function projected each one of the output coordinates. Finally, the $\cdot$ on the right hand side is good old matrix multiplication.</p><p>It's important to point out that usually the vectors spaces $V$ and $W$ will usually have extra structure, they will be tensors of some shape and even though that doesn't affect the chain rule derivation described above, that structure will come in handy in the gradient calculation. The Jacobian can be huge, but very often it will be a sparse, <a href="https://en.wikipedia.org/wiki/Block_matrix">block</a> or diagonal matrix and we wont't need to compute it explicitly, because we only care about the result of multiplying it by the gradient. This operation is sometimes referred in the literature as <em>VJP</em>, or Vector-Jacobian Product.</p><h2 id="link-to-deep-neural-networks">Link to deep neural networks</h2><p>In the typical setting for supervised machine learning we have a big complex function that takes in a tensor of numerical features for our labelled samples, and several tensors that correspond to weights that characterize the model, and the output is a loss we want to minimize to find the most suitable weights. In deep learning, this complex function is expressed as a composition of simpler functions, which are much easier to differentiate and reason about.</p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>A two-layer fully connected neural network</figcaption></figure><p>For example a linear layer $f$ is characterized by a matrix multiplication (let's leave the bias out for a sec):</p><p>$f(X, w) = X\cdot w$</p><p>If this layer is the last one of the model, after that we compute the scalar loss, comparing the predicted outputs $y_p$ with the true ones $y_t$ with, for instance, a mean square error metric. In order to tune the values of $w$ we need to compute the gradient $\nabla_w g \circ f$ with respect to $w$. Applying the chain rule, it's enough to calculate $\nabla g$ which is $2\  \text{avg}(y_p - y_t)$ and $J_w f$ with respect $w$, which is $X^t$, since $f$ is linear with respect to $w$. We can similarly calculate $J_X f$ with respect to the input $X$ to be $w$, and this allows us to compute $\nabla_X	 g \circ f$ with respect to $X$, ie: the gradient of the loss with respect to the layer inputs, this isn't useful at first sight, but wait and see!</p><p><strong>So where does the back-prop actually happen?</strong></p><p>When we want to go back and tune the weights of the second to last layer. If we call $h$ to this layer we will need $J h$ and $\nabla_X g \circ f$, to calculate the gradient with respect to the layer weights using the chain rule, and also $\nabla h \circ g \circ f$ to keep passing backwards, and that's it!</p><h2 id="finally-it-s-time-to-implement-it">Finally, it's time to implement it</h2><p>Dependency wise, we only need <code>numpy</code> and <code>matplotlib</code> to visualize the learning progress later on</p><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
</code></pre>
<p>We can start with a class that encapsulates a tensor and its gradients</p><pre><code class="language-python">class Parameter():
  def __init__(self, tensor):
    self.tensor = tensor
    self.gradient = np.zeros_like(self.tensor)
</code></pre>
<p>Now we can create the layer class, the key idea is that in forward pass we return both the layer output and function that can receive the gradient of loss with respect to the outputs can return the gradient with respect of the inputs, and update the weight gradients in the process.</p><p>The training process will then have three steps, calculate the forward step, then the backwards steps accumulates the gradients, and finally updating the weights. It’s important to this at the end, since weights can be reused in multiple layers and we don’t want to mutate the weights before time.</p><pre><code class="language-python">class Layer:
  def __init__(self):
    self.parameters = []

  def forward(self, X):
    """
    Override me! A simple no-op layer, it passes forward the inputs
    """
    return X, lambda D: D

  def build_param(self, tensor):
    """
    Creates a parameter from a tensor, and saves a reference for the update step
    """
    param = Parameter(tensor)
    self.parameters.append(param)
    return param

  def update(self, optimizer):
    for param in self.parameters: optimizer.update(param)
</code></pre>
<p>It's standard to delegate the job of updating the parameters to an optimizer, which receives an instance of a parameter after every batch. The simplest and most known optimization method out there is the mini-batch stochastic gradient descent</p><pre><code class="language-python">class SGDOptimizer():
  def __init__(self, lr=0.1):
    self.lr = lr

  def update(self, param):
    param.tensor -= self.lr * param.gradient
    param.gradient.fill(0)
</code></pre>
<p>Under this framework, the linear layer looks like this:</p><pre><code class="language-python">class Linear(Layer):
  def __init__(self, inputs, outputs):
    super().__init__()
    tensor = np.random.randn(inputs, outputs) * np.sqrt(1 / inputs)
    self.weights = self.build_param(tensor)
    self.bias = self.build_param(np.zeros(outputs))

  def forward(self, X):
    def backward(D):
      self.weights.gradient += X.T @ D
      self.bias.gradient += D.sum(axis=0)
      return D @ self.weights.tensor.T
    return X @ self.weights.tensor +  self.bias.tensor, backward
</code></pre>
<p>Now, the next most used types of layers are activations, which are non-linear point-wise functions. The Jacobian of a point-wise function is diagonal, which means that when multiplied by the gradient it also acts a point-wise multiplication. </p><pre><code class="language-python">class ReLu(Layer):
  def forward(self, X):
    mask = X &gt; 0
    return X * mask, lambda D: D * mask
</code></pre>
<p>Now that I have we have two kinds of layers, we want to compose them and make sure that backward propagation works as expected. As we traverse the layers and get the successive outputs we can save the <code>backward</code> functions in a list that will be used in the reverse order, to get all the way to gradient with respect to the first layer inputs and return it. This is were the magic happens:</p><pre><code class="language-python">class Sequential(Layer):
  def __init__(self, *layers):
    super().__init__()
    self.layers = layers
    for layer in layers:
      self.parameters.extend(layer.parameters)

  def forward(self, X):
    backprops = []
    Y = X
    for layer in self.layers:
      Y, backprop = layer.forward(Y)
      backprops.append(backprop)
    def backward(D):
      for backprop in reversed(backprops): 
        D = backprop(D)
      return D
    return Y, backward
</code></pre>
<p>As we mentioned earlier, we will need a way to calculate the loss function associated with a batch of samples, and its gradient. One example would be <em>MSE</em> loss, typically used in regression problems, and it can be implemented in this manner:</p><pre><code class="language-python">def mse_loss(Yp, Yt):
  diff = Yp - Yt
  return np.square(diff).mean(), 2 * diff / len(diff)
</code></pre>
<p>Almost there now, we have two types of layers and a way to combine them, so how does the training loop look like. We can use an API similar to <code>scikit-learn</code> or <code>keras</code>.</p><pre><code class="language-python">class Learner():
  def __init__(self, model, loss, optimizer):
    self.model = model
    self.loss = loss
    self.optimizer = optimizer

  def fit_batch(self, X, Y):
    Y_, backward = self.model.forward(X)
    L, D = self.loss(Y_, Y)
    backward(D)
    self.model.update(self.optimizer)
    return L

  def fit(self, X, Y, epochs, bs):
    losses = []
    for epoch in range(epochs):
      p = np.random.permutation(len(X))
      X, Y = X[p], Y[p]
      loss = 0.0
      for i in range(0, len(X), bs):
        loss += self.fit_batch(X[i:i + bs], Y[i:i + bs])
      losses.append(loss)
      print('Epoch', epoch, '\tLoss', loss)
    return losses
</code></pre>
<p>And that's it, if you were keeping track, we even have a few lines of code to spare. </p><h2 id="but-does-it-work">But does it work?</h2><p>Thought you would never ask, we can start testing with some synthetic data.</p><pre><code class="language-python">X = np.random.randn(100, 10)
w = np.random.randn(10, 1)
b = np.random.randn(1)
Y = X @ W + B

model = Linear(10, 1)
learner = Learner(model, mse_loss, SGDOptimizer(lr=0.05))
learner.fit(X, Y, epochs=10, bs=10)
</code></pre>
<figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>Training loss in 10 epochs</figcaption></figure><p>We can also check that the learned weights coincide with the true ones</p><pre><code class="language-python">print(np.linalg.norm(m.weights.tensor - W), (m.bias.tensor - B)[0])
&gt; 1.848553648022619e-05 5.69305886743976e-06
</code></pre>
<p>Ok, so that was easy, but let's come up with a non-linear dataset, how about $y = x_1 x_2$. Here we go:</p><p><br /></p>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../index.html">Julian Eisenschlos</a> &copy; 2019</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
</html>
